{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö° Spark Fundamentals\n",
    "\n",
    "**Phase 2 (Intermediate) - Module 5 of 6**\n",
    "\n",
    "**Estimated time**: 90-120 minutes\n",
    "\n",
    "**Prerequisites**: [04_Testing_with_ScalaTest.ipynb](04_Testing_with_ScalaTest.ipynb)\n",
    "\n",
    "## üéØ Learning Goals\n",
    "\n",
    "- Understand Apache Spark architecture and principles\n",
    "- Work with RDDs, DataFrames, and Datasets\n",
    "- Master Spark SQL and DataFrame operations\n",
    "- Implement distributed transformations and actions\n",
    "- Handle data partitioning and caching\n",
    "- Build complete Spark applications\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Table of Contents\n",
    "\n",
    "1. [Spark Overview](#overview)\n",
    "2. [RDD Operations](#rdd)\n",
    "3. [DataFrames & Spark SQL](#dataframe)\n",
    "4. [Datasets](#datasets)\n",
    "5. [Data Partitioning](#partitioning)\n",
    "6. [Caching & Persistence](#caching)\n",
    "7. [Exercises](#exercises)\n",
    "8. [What Next](#next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Why Apache Spark?\n",
    "\n",
    "**Data Processing Evolution:**\n",
    "\n",
    "**Traditional:** Single machine ‚Üí Memory & storage limits\n",
    "**Hadoop MapReduce:** Cluster processing ‚Üí But slow (disk I/O)\n",
    "**Apache Spark:** In-memory processing ‚Üí **100x faster**\n",
    "\n",
    "**Spark solves:**\n",
    "- **Big Data processing** at scale\n",
    "- **Real-time analytics** and batch processing\n",
    "- **Complex data workflows** (ML, SQL, streaming)\n",
    "- **Fault tolerance** with automatic recovery\n",
    "- **Multi-language support** (Scala, Python, Java, R)\n",
    "\n",
    "**Key Features:**\n",
    "- **In-memory computing**: Keep data in RAM\n",
    "- **Lazy evaluation**: Optimize execution plans\n",
    "- **DAG execution**: Efficient task scheduling\n",
    "- **Rich APIs**: Functional programming with collections\n",
    "- **Unified platform**: Spark SQL, MLlib, Streaming, GraphX\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Spark Setup\n",
    "\n",
    "Setting up Spark environment and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// In a real Spark application, add dependencies:\n",
    "// libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"3.3.2\"\n",
    "// libraryDependencies += \"org.apache.spark\" %% \"spark-sql\" % \"3.3.2\"\n",
    "\n",
    "// For this notebook, we'll simulate Spark concepts\n",
    "println(\"=== Spark Environment Setup ===\")\n",
    "println(\"In a real project:\")\n",
    "println(\"1. Add Spark dependencies to build.sbt\")\n",
    "println(\"2. Configure spark-defaults.conf\")\n",
    "println(\"3. Set up master URL (local[*], yarn, k8s, etc.)\")\n",
    "println(\"4. Create SparkSession\")\n",
    "\n",
    "println(\"\\nSpark Architecture:\")\n",
    "println(\"‚úì Driver Program: Main application\")\n",
    "println(\"‚úì Cluster Manager: Allocate resources (YARN, Kubernetes, Mesos)\")\n",
    "println(\"‚úì Worker Nodes: Execute tasks\")\n",
    "println(\"‚úì Executors: Run tasks on worker nodes\")\n",
    "println()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è RDD Fundamentals\n",
    "\n",
    "Resilient Distributed Datasets - Spark's core abstraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Simulate RDD concepts (in real Spark, these would be distributed)\n",
    "println(\"=== RDD Basics ===\")\n",
    "\n",
    "// Simulate creating RDDs\n",
    "object SimRDD {\n",
    "  def makeRDD[T](data: Seq[T]): SimRDD[T] = new SimRDD(data)\n",
    "\n",
    "  def textFile(path: String): SimRDD[String] = {\n",
    "    val simulatedData = List(\n",
    "      \"Spark is fast\",\n",
    "      \"RDDs are immutable\",\n",
    "      \"Transformations are lazy\",\n",
    "      \"Actions trigger execution\"\n",
    "    )\n",
    "    new SimRDD(simulatedData)\n",
    "  }\n",
    "}\n",
    "\n",
    "class SimRDD[T](private val data: Seq[T]) {\n",
    "  // Transformations (lazy)\n",
    "  def map[U](f: T => U): SimRDD[U] = {\n",
    "    println(s\"MAP transformation (lazy): $f\")\n",
    "    new SimRDD(data.map(f))\n",
    "  }\n",
    "\n",
    "  def filter(f: T => Boolean): SimRDD[T] = {\n",
    "    println(s\"FILTER transformation (lazy): $f\")\n",
    "    new SimRDD(data.filter(f))\n",
    "  }\n",
    "\n",
    "  def flatMap[U](f: T => Seq[U]): SimRDD[U] = {\n",
    "    println(s\"FLATMAP transformation (lazy): $f\")\n",
    "    new SimRDD(data.flatMap(f))\n",
    "  }\n",
    "\n",
    "  // Actions (trigger computation)\n",
    "  def collect(): Seq[T] = {\n",
    "    println(\"COLLECT action (executes all transformations)\")\n",
    "    data\n",
    "  }\n",
    "\n",
    "  def count(): Int = {\n",
    "    println(\"COUNT action (executes transformations)\")\n",
    "    data.size\n",
    "  }\n",
    "}\n",
    "\n",
    "// Demonstrate RDD operations\n",
    "val numbersRDD = SimRDD.makeRDD(Seq(1, 2, 3, 4, 5, 6, 7, 8, 9, 10))\n",
    "\n",
    "println(\"Creating pipeline (lazy - no execution yet):\")\n",
    "val resultRDD = numbersRDD\n",
    "  .filter(_ % 2 == 0)    // Even numbers only\n",
    "  .map(_ * 2)            // Double them\n",
    "  .filter(_ > 5)         // Greater than 5\n",
    "\n",
    "println(\"\\nAction triggers execution:\")\n",
    "val result = resultRDD.collect()\n",
    "println(s\"Final result: ${result.mkString(\", \")}\")\n",
    "println()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Word Count Example\n",
    "\n",
    "Classic Spark example showing transformations and actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Word count with simulated RDD\n",
    "println(\"=== Word Count Example ===\")\n",
    "\n",
    "val linesRDD = SimRDD.textFile(\"sample.txt\")\n",
    "\n",
    "// Word count pipeline\n",
    "val wordCounts = linesRDD\n",
    "  .flatMap(_.split(\"\\\\s+\"))        // Split into words\n",
    "  .map(_.toLowerCase)              // Normalize case\n",
    "  .map(word => (word, 1))          // Create pairs\n",
    "  .groupBy(_._1)                   // Group by word\n",
    "  .map { case (word, pairs) =>     // Sum counts\n",
    "    (word, pairs.size)\n",
    "  }\n",
    "\n",
    "println(\"Word count pipeline created (lazy)\")\n",
    "\n",
    "// Collect results\n",
    "val counts = wordCounts.collect()\n",
    "println(\"\\nWord counts:\")\n",
    "counts.sortBy(-_._2).foreach { case (word, count) =>\n",
    "  println(f\"  $word%-10s : $count\")\n",
    "}\n",
    "\n",
    "println(f\"\\nTotal unique words: ${counts.size}\")\n",
    "println(f\"Total words: ${counts.map(_._2).sum}\")\n",
    "println()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã DataFrames & Spark SQL\n",
    "\n",
    "Higher-level abstraction with SQL-like operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Simulated DataFrame operations\n",
    "println(\"=== DataFrames & Spark SQL ===\")\n",
    "\n",
    "case class Employee(id: Int, name: String, department: String, salary: Double)\n",
    "\n",
    "// Simulate Spark DataFrame-like operations\n",
    "object SimDataFrame {\n",
    "  def fromData[T](data: Seq[T]): SimDataFrame[T] = new SimDataFrame[T](data)\n",
    "}\n",
    "\n",
    "class SimDataFrame[T](private val data: Seq[T]) {\n",
    "  def filter(f: T => Boolean): SimDataFrame[T] = {\n",
    "    println(s\"FILTER: $f\")\n",
    "    new SimDataFrame(data.filter(f))\n",
    "  }\n",
    "\n",
    "  def map[U](f: T => U): SimDataFrame[U] = {\n",
    "    println(s\"MAP: $f\")\n",
    "    new SimDataFrame(data.map(f))\n",
    "  }\n",
    "\n",
    "  def select[U](f: T => U): SimDataFrame[U] = map(f)\n",
    "\n",
    "  def groupBy[K](f: T => K): Map[K, Seq[T]] = {\n",
    "    println(s\"GROUPBY: $f\")\n",
    "    data.groupBy(f)\n",
    "  }\n",
    "\n",
    "  def agg[K, V: Numeric](groupByF: T => K)(aggF: Seq[T] => V): Map[K, V] = {\n",
    "    println(s\"AGGREGATE via $groupByF\")\n",
    "    val grouped = data.groupBy(groupByF)\n",
    "    grouped.map { case (k, vs) => k -> aggF(vs) }\n",
    "  }\n",
    "\n",
    "  def collect(): Seq[T] = {\n",
    "    println(\"COLLECT action\")\n",
    "    data\n",
    "  }\n",
    "\n",
    "  def show(): Unit = {\n",
    "    println(\"=== DataFrame Preview ===\")\n",
    "    data.take(5).foreach(println)\n",
    "    if (data.size > 5) println(s\"... (${data.size - 5} more rows)\")\n",
    "  }\n",
    "}\n",
    "\n",
    "// Employee data\n",
    "val employees = Seq(\n",
    "  Employee(1, \"Alice\", \"Engineering\", 75000),\n",
    "  Employee(2, \"Bob\", \"Engineering\", 80000),\n",
    "  Employee(3, \"Charlie\", \"Sales\", 65000),\n",
    "  Employee(4, \"Diana\", \"Sales\", 70000),\n",
    "  Employee(5, \"Eve\", \"HR\", 60000),\n",
    "  Employee(6, \"Frank\", \"Engineering\", 85000)\n",
    ")\n",
    "\n",
    "val employeeDF = SimDataFrame.fromData(employees)\n",
    "\n",
    "// DataFrame operations\n",
    "employeeDF.show()\n",
    "\n",
    "println(\"\\n=== Analysis Operations ===\")\n",
    "\n",
    "// Filter\n",
    "val engineers = employeeDF.filter(_.department == \"Engineering\")\n",
    "println(f\"\\nEngineers: ${engineers.collect().size}\")\n",
    "\n",
    "// Group and aggregate\n",
    "val deptSalaries = employeeDF.agg(_.department)(_.map(_.salary).sum)\n",
    "println(\"\\nDepartment salaries:\")\n",
    "deptSalaries.foreach { case (dept, total) =>\n",
    "  println(f\"  $dept%-12s : $$$total%,.0f\")\n",
    "}\n",
    "\n",
    "// Average salary by department\n",
    "val deptAvg = employeeDF.agg(_.department) { employees =>\n",
    "  employees.map(_.salary).sum / employees.size\n",
    "}\n",
    "println(\"\\nDepartment average salaries:\")\n",
    "deptAvg.foreach { case (dept, avg) =>\n",
    "  println(f\"  $dept%-12s : $$$avg%,.0f\")\n",
    "}\n",
    "println()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèõÔ∏è Datasets\n",
    "\n",
    "Type-safe operations with case classes and implicit conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Datasets - strongly typed API\n",
    "println(\"=== Dataset Operations ===\")\n",
    "\n",
    "// Type-safe Dataset simulation\n",
    "class SimDataset[T: Manifest](private val data: Seq[T]) {\n",
    "  def filter(f: T => Boolean): SimDataset[T] = {\n",
    "    new SimDataset(data.filter(f))\n",
    "  }\n",
    "\n",
    "  def map[U: Manifest](f: T => U): SimDataset[U] = {\n",
    "    new SimDataset(data.map(f))\n",
    "  }\n",
    "\n",
    "  def flatMap[U: Manifest](f: T => Seq[U]): SimDataset[U] = {\n",
    "    new SimDataset(data.flatMap(f))\n",
    "  }\n",
    "\n",
    "  // Type-safe aggregation\n",
    "  def groupByKey[K: Manifest](keyFunc: T => K): SimGroupedDataset[K, T] = {\n",
    "    new SimGroupedDataset(data.groupBy(keyFunc))\n",
    "  }\n",
    "\n",
    "  def collect(): Seq[T] = data\n",
    "  def count(): Long = data.size\n",
    "}\n",
    "\n",
    "class SimGroupedDataset[K, T](private val grouped: Map[K, Seq[T]]) {\n",
    "  def count(): SimDataset[(K, Long)] = {\n",
    "    new SimDataset(grouped.map { case (k, v) => (k, v.size.toLong) }.toSeq)\n",
    "  }\n",
    "\n",
    "  def sum[U: Numeric](valueFunc: T => U): SimDataset[(K, U)] = {\n",
    "    new SimDataset(grouped.map { case (k, vs) =>\n",
    "      (k, vs.map(valueFunc).sum(implicitly[Numeric[U]]))\n",
    "    }.toSeq)\n",
    "  }\n",
    "\n",
    "  def avg[U: Numeric](valueFunc: T => U): SimDataset[(K, Double)] = {\n",
    "    new SimDataset(grouped.map { case (k, vs) =>\n",
    "      val values = vs.map(valueFunc)\n",
    "      val sum = values.sum(implicitly[Numeric[U]]).toDouble\n",
    "      val count = values.size.toDouble\n",
    "      (k, sum / count)\n",
    "    }.toSeq)\n",
    "  }\n",
    "}\n",
    "\n",
    "// Using Datasets\n",
    "val employeeDS = new SimDataset(employees)\n",
    "\n",
    "println(\"Dataset Operations:\")\n",
    "\n",
    "// Type-safe filtering\n",
    "val highPaidDS = employeeDS.filter(_.salary > 70000)\n",
    "println(s\"\\nHigh-paid employees: ${highPaidDS.count()}\")\n",
    "\n",
    "// Type-safe aggregation\n",
    "val deptCount = employeeDS.groupByKey(_.department).count()\n",
    "println(\"\\nEmployee count by department:\")\n",
    "deptCount.collect().foreach { case (dept, count) =>\n",
    "  println(s\"  $dept: $count\")\n",
    "}\n",
    "\n",
    "val deptTotalSalary = employeeDS.groupByKey(_.department).sum(_.salary)\n",
    "println(\"\\nTotal salary by department:\")\n",
    "deptTotalSalary.collect().foreach { case (dept, total) =>\n",
    "  println(f\"  $dept: $$$total%,.0f\")\n",
    "}\n",
    "\n",
    "println(\"\\nDatasets provide compile-time type safety!\")\n",
    "println()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÄ Data Partitioning\n",
    "\n",
    "Controlling data distribution for performance optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Data partitioning concepts\n",
    "println(\"=== Data Partitioning ===\")\n",
    "\n",
    "// Partition simulator\n",
    "class SimPartitioner(private val partitions: Seq[Seq[String]]) {\n",
    "  def getPartition(key: String): Int = {\n",
    "    key.hashCode % partitions.size\n",
    "  }\n",
    "\n",
    "  def repartition(newNumPartitions: Int, data: Seq[String]): SimPartitioner = {\n",
    "    println(s\"REPARTITION: ${partitions.size} -> $newNumPartitions partitions\")\n",
    "    val newPartitions = (0 until newNumPartitions).map { i =>\n",
    "      data.filter(item => item.hashCode % newNumPartitions == i)\n",
    "    }\n",
    "    new SimPartitioner(newPartitions)\n",
    "  }\n",
    "\n",
    "  def analyze(): Unit = {\n",
    "    println(\"Partition Analysis:\")\n",
    "    partitions.zipWithIndex.foreach { case (data, idx) =>\n",
    "      println(f\"  Partition $idx: ${data.size}%3d items\")\n",
    "    }\n",
    "    val totalItems = partitions.map(_.size).sum\n",
    "    val avgItems = totalItems.toDouble / partitions.size\n",
    "    val maxItems = partitions.map(_.size).max\n",
    "    val minItems = partitions.map(_.size).min\n",
    "    println(f\"  Total items: $totalItems\")\n",
    "    println(f\"  Average per partition: $avgItems%.1f\")\n",
    "    println(f\"  Data skew: ${maxItems - minItems} (max $maxItems, min $minItems)\")\n",
    "  }\n",
    "}\n",
    "\n",
    "// Create some test data\n",
    "val testData = Seq(\"apple\", \"banana\", \"cherry\", \"date\",\n",
    "                   \"apple\", \"banana\", \"cherry\", \"date\",\n",
    "                   \"fig\", \"grape\", \"honeydew\", \"kiwi\")\n",
    "\n",
    "// Initial partitioning\n",
    "val initialPartitions = (0 until 3).map { i =>\n",
    "  testData.filter(item => item.hashCode % 3 == i)\n",
    "}\n",
    "\n",
    "val partitioner = new SimPartitioner(initialPartitions)\n",
    "println(\"Initial partitioning:\")\n",
    "partitioner.analyze()\n",
    "\n",
    "println(\"\\nRepartitioning to 4 partitions:\")\n",
    "val repartitioned = partitioner.repartition(4, testData)\n",
    "repartitioned.analyze()\n",
    "\n",
    "println(\"\\n=== Partitioning Strategies ===\")\n",
    "println(\"‚úì Hash Partitioning: key.hashCode % numPartitions\")\n",
    "println(\"‚úì Range Partitioning: sorted ranges of keys\")\n",
    "println(\"‚úì Custom Partitioning: business logic based\")\n",
    "println()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÜ Exercises\n",
    "\n",
    "### Exercise 1: RDD Operations\n",
    "\n",
    "Implement common RDD operations and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Exercise 1: RDD Operations\n",
    "// FIXME: Replace ??? with your code\n",
    "\n",
    "class EnhancedRDD[T](data: Seq[T]) extends SimRDD[T](data) {\n",
    "  def distinct: EnhancedRDD[T] = {\n",
    "    new EnhancedRDD(data.distinct)\n",
    "  }\n",
    "\n",
    "  def union(other: EnhancedRDD[T]): EnhancedRDD[T] = {\n",
    "    new EnhancedRDD(data ++ other.collect())\n",
    "  }\n",
    "\n",
    "  def intersection(other: EnhancedRDD[T]): EnhancedRDD[T] = {\n",
    "    new EnhancedRDD(data.intersect(other.collect()))\n",
    "  }\n",
    "\n",
    "  def sortBy[U: Ordering](f: T => U): EnhancedRDD[T] = {\n",
    "    new EnhancedRDD(data.sortBy(f))\n",
    "  }\n",
    "\n",
    "  def take(n: Int): Seq[T] = data.take(n)\n",
    "\n",
    "  def first(): Option[T] = data.headOption\n",
    "\n",
    "  def zipWithIndex: EnhancedRDD[(T, Long)] = {\n",
    "    new EnhancedRDD(data.zipWithIndex.map { case (t, i) => (t, i.toLong) })\n",
    "  }\n",
    "}\n",
    "\n",
    "// Test RDD operations\n",
    "println(\"RDD Operations Exercise:\")\n",
    "println(\"=\" * 25)\n",
    "\n",
    "val numbers = new EnhancedRDD(Seq(3, 1, 4, 1, 5, 9, 2, 6))\n",
    "val moreNumbers = new EnhancedRDD(Seq(1, 2, 7, 8))\n",
    "\n",
    "println(\"Original: \" + numbers.collect().mkString(\", \"))\n",
    "println(\"Distinct: \" + numbers.distinct.collect().mkString(\", \"))\n",
    "println(\"Union: \" + numbers.union(moreNumbers).collect().mkString(\", \"))\n",
    "println(\"Intersection: \" + numbers.intersection(moreNumbers).collect().mkString(\", \"))\n",
    "println(\"Sorted: \" + numbers.sortBy(identity).collect().mkString(\", \"))\n",
    "println(\"First 3: \" + numbers.take(3).mkString(\", \"))\n",
    "println(\"First: \" + numbers.first().getOrElse(\"empty\"))\n",
    "\n",
    "val withIndices = numbers.zipWithIndex.collect()\n",
    "println(\"With indices: \" + withIndices.map{case (n,i)=>s\"$n->$i\"}.mkString(\", \"))\n",
    "\n",
    "println()\n",
    "\n",
    "// Word processing operations\n",
    "val words = new EnhancedRDD(Seq(\"hello\", \"world\", \"hello\", \"spark\", \"big\", \"data\"))\n",
    "val moreWords = new EnhancedRDD(Seq(\"hello\", \"distributed\", \"computing\"))\n",
    "\n",
    "println(\"Word analysis:\")\n",
    "println(\"Words: \" + words.collect().mkString(\", \"))\n",
    "println(\"Distinct words: \" + words.distinct.collect().mkString(\", \"))\n",
    "println(\"Combined unique: \" + words.union(moreWords).distinct.collect().sorted.mkString(\", \"))\n",
    "println(\"Common words: \" + words.intersection(moreWords).collect().mkString(\", \"))\n",
    "println(\"Alphabetical: \" + words.sortBy(identity).collect().mkString(\", \"))\n",
    "println()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: DataFrame Analytics\n",
    "\n",
    "Perform data analysis operations on employee dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Exercise 2: DataFrame Analytics\n",
    "// FIXME: Replace ??? with your code\n",
    "\n",
    "val analyticsDF = SimDataFrame.fromData(employees)\n",
    "\n",
    "println(\"DataFrame Analytics Exercise:\")\n",
    "println(\"=\" * 32)\n",
    "\n",
    "// Basic statistics\n",
    "val totalEmployees = analyticsDF.collect().size\n",
    "val totalSalary = analyticsDF.collect().map(_.salary).sum\n",
    "val avgSalary = totalSalary / totalEmployees\n",
    "val highestSalary = analyticsDF.collect().map(_.salary).max\n",
    "val lowestSalary = analyticsDF.collect().map(_.salary).min\n",
    "\n",
    "println(\"Overall Statistics:\")\n",
    "println(f\"  Total Employees: $totalEmployees\")\n",
    "println(f\"  Total Salary Budget: $$$totalSalary%,.0f\")\n",
    "println(f\"  Average Salary: $$$avgSalary%.0f\")\n",
    "println(f\"  Salary Range: $$$lowestSalary%,.0f - $$$highestSalary%,.0f\")\n",
    "println()\n",
    "\n",
    "// Department statistics\n",
    "val deptStats = analyticsDF\n",
    "  .agg(_.department) { emps =>\n",
    "    val salaries = emps.map(_.salary)\n",
    "    (emps.size, salaries.sum, salaries.max, salaries.min, salaries.sum / emps.size)\n",
    "  }\n",
    "\n",
    "println(\"Department Analytics:\")\n",
    "deptStats.foreach { case (dept, (count, total, max, min, avg)) =>\n",
    "  println(f\"  $dept%-12s: $count%2d employees, total $$$total%7,.0f, avg $$$avg%6.0f\")\n",
    "  println(f\"                   Salary range: $$$min%,.0f - $$$max%,.0f\")\n",
    "}\n",
    "\n",
    "println()\n",
    "println(\"Highest paid by department:\")\n",
    "deptStats.keys.foreach { dept =>\n",
    "  val deptEmps = analyticsDF.collect().filter(_.department == dept)\n",
    "  val highest = deptEmps.maxBy(_.salary)\n",
    "  println(f\"  $dept%-12s: ${highest.name} ($$${highest.salary}%,.0f)\")\n",
    "}\n",
    "\n",
    "println()\n",
    "println(\"Spark DataFrames enable complex analytics with simple APIs!\")\n",
    "println()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù What Next?\n",
    "\n",
    "üéâ **Congratulations!** You've mastered Spark Fundamentals!\n",
    "\n",
    "**You've learned:**\n",
    "- Apache Spark architecture and principles\n",
    "- RDDs: Spark's core abstraction\n",
    "- DataFrames: Higher-level operations\n",
    "- Datasets: Type-safe operations\n",
    "- Data partitioning for performance\n",
    "- Distributed computing concepts\n",
    "\n",
    "**Key Concepts:**\n",
    "- **RDD**: Immutable, distributed collections\n",
    "- **Transformations**: Lazy operations\n",
    "- **Actions**: Trigger execution\n",
    "- **DataFrames**: SQL-like operations\n",
    "- **Datasets**: Type-safe DataFrames\n",
    "- **Partitioning**: Data distribution strategy\n",
    "\n",
    "**Next Steps:**\n",
    "1. Complete exercises - experiment with all APIs\n",
    "2. Move to **06: Macros & Metaprogramming**\n",
    "3. Set up real Spark cluster for hands-on experience\n",
    "4. Explore Spark Streaming and MLlib\n",
    "\n",
    "**Production Tips:**\n",
    "- Monitor partition balance (data skew hurts performance)\n",
    "- Cache frequently used data\n",
    "- Use appropriate storage levels\n",
    "- Tune memory allocation\n",
    "- Monitor job execution in Spark UI\n",
    "\n",
    "**Real-world Spark projects:**\n",
    "- ETL pipelines\n",
    "- Real-time analytics\n",
    "- Machine learning at scale\n",
    "- Graph processing\n",
    "\n",
    "---\n",
    "\n",
    "*\"Spark is the Tesla of Big Data: faster, smarter, and more beautiful.\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
