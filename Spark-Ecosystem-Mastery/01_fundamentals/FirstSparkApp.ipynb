{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ First Spark Application\n",
    "\n",
    "**Phase 1 (Fundamentals) - Your First Spark Program**\n",
    "\n",
    "**Prerequisites**: Basic Scala knowledge, Docker/Spark installation\n",
    "\n",
    "**Estimated time**: 30-45 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Goals\n",
    "\n",
    "By the end of this notebook, you'll be able to:\n",
    "- âœ… Create and configure a SparkSession\n",
    "- âœ… Work with Resilient Distributed Datasets (RDDs)\n",
    "- âœ… Perform basic transformations and actions\n",
    "- âœ… Understand lazy evaluation in Spark\n",
    "- âœ… Build a simple word count application\n",
    "- âœ… Monitor Spark applications\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Table of Contents\n",
    "\n",
    "1. [SparkSession Setup](#setup)\n",
    "2. [Basic RDD Operations](#rdd)\n",
    "3. [Transformations vs Actions](#transform)\n",
    "4. [Word Count Application](#wordcount)\n",
    "5. [Monitoring & Debugging](#monitoring)\n",
    "6. [Exercises](#exercises)\n",
    "7. [Next Steps](#next)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ SparkSession Setup\n",
    "\n",
    "**SparkSession is the entry point for all Spark functionality.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Import Spark libraries\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create SparkSession\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"First Spark Application\")\n",
    "  .master(\"local[*]\")  // Use all available cores\n",
    "  .getOrCreate()\n",
    "\n",
    "// Get SparkContext (for RDD operations)\n",
    "val sc = spark.sparkContext\n",
    "\n",
    "println(\"ðŸš€ Spark Application Started!\")\n",
    "println(s\"ðŸ”¥ Spark Version: ${spark.version}\")\n",
    "println(s\"ðŸŽ¯ Master: ${sc.master}\")\n",
    "println(s\"âš¡ Available Cores: ${sc.defaultParallelism}\")\n",
    "println(s\"ðŸ“Š Application ID: ${sc.applicationId}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened?**\n",
    "\n",
    "- **`SparkSession.builder()`**: Creates a builder for SparkSession\n",
    "- **`.appName()`**: Sets application name (shows in UI)\n",
    "- **`.master()`**: Specifies cluster manager (`local[*]` = local mode)\n",
    "- **`.getOrCreate()`**: Gets existing session or creates new one\n",
    "- **`spark.sparkContext`**: Access to lower-level RDD API\n",
    "\n",
    "**Key Points:**\n",
    "- Always create SparkSession at the beginning\n",
    "- `local[*]` uses all available CPU cores\n",
    "- Spark UI will be available at http://localhost:4040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¢ Basic RDD Operations\n",
    "\n",
    "**RDD (Resilient Distributed Dataset) is Spark's core abstraction for distributed data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create RDD from a collection\n",
    "val numbers = Seq(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
    "val numbersRDD = sc.parallelize(numbers)\n",
    "\n",
    "println(\"ðŸ“Š Original Data:\")\n",
    "println(s\"Numbers: ${numbers.mkString(\", \")}\")\n",
    "println(s\"RDD partitions: ${numbersRDD.getNumPartitions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Basic RDD operations\n",
    "println(\"ðŸ”„ RDD Transformations:\")\n",
    "\n",
    "// Filter even numbers\n",
    "val evenNumbers = numbersRDD.filter(_ % 2 == 0)\n",
    "println(s\"Even numbers: ${evenNumbers.collect().mkString(\", \")}\")\n",
    "\n",
    "// Square all numbers\n",
    "val squaredNumbers = numbersRDD.map(x => x * x)\n",
    "println(s\"Squared numbers: ${squaredNumbers.collect().mkString(\", \")}\")\n",
    "\n",
    "// Filter numbers greater than 5, then double them\n",
    "val processedNumbers = numbersRDD\n",
    "  .filter(_ > 5)\n",
    "  .map(_ * 2)\n",
    "println(s\"Numbers > 5, doubled: ${processedNumbers.collect().mkString(\", \")}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Aggregation operations (Actions)\n",
    "println(\"ðŸ“ˆ RDD Actions:\")\n",
    "println(s\"Count: ${numbersRDD.count()}\")\n",
    "println(s\"Sum: ${numbersRDD.sum()}\")\n",
    "println(f\"Average: ${numbersRDD.mean()}%.2f\")\n",
    "println(s\"Min: ${numbersRDD.min()}\")\n",
    "println(s\"Max: ${numbersRDD.max()}\")\n",
    "println(s\"First 3 elements: ${numbersRDD.take(3).mkString(\", \")}\")\n",
    "println(s\"Sample (20%): ${numbersRDD.takeSample(false, 2).mkString(\", \")}\")"
   ]
  },
  {
   "markdown": {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "**Key RDD Operations:**\n",
     "\n",
     "| Operation | Type | Description | Example |\n",
     "|-----------|------|-------------|---------|\n",
     "| `map()` | Transformation | Apply function to each element | `rdd.map(_ * 2)` |\n",
     "| `filter()` | Transformation | Keep elements matching predicate | `rdd.filter(_ > 5)` |\n",
     "| `flatMap()` | Transformation | Apply function, flatten results | `rdd.flatMap(_.split(\" \"))` |\n",
     "| `collect()` | Action | Return all elements as array | `rdd.collect()` |\n",
     "| `count()` | Action | Count elements | `rdd.count()` |\n",
     "| `take(n)` | Action | Return first n elements | `rdd.take(5)` |\n",
     "| `reduce()` | Action | Combine elements | `rdd.reduce(_ + _)` |\n",
     "\n",
     "**Important:** Transformations are lazy (not executed immediately), Actions trigger computation!"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ Transformations vs Actions\n",
    "\n",
    "**Understanding lazy evaluation is crucial in Spark!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Demonstrate lazy evaluation\n",
    "println(\"ðŸ¦¥ Lazy Transformations (no computation yet):\")\n",
    "\n",
    "val lazyRDD = numbersRDD\n",
    "  .filter(_ > 5)\n",
    "  .map(_ * 2)\n",
    "  .filter(_ < 18)\n",
    "\n",
    "println(\"âœ“ Transformations defined but not executed\")\n",
    "println(s\"RDD type: ${lazyRDD.getClass.getSimpleName}\")\n",
    "println(s\"RDD partitions: ${lazyRDD.getNumPartitions}\")\n",
    "\n",
    "// Now trigger computation with an action\n",
    "println(\"\\nâš¡ Action Triggers Computation:\")\n",
    "val result = lazyRDD.collect()\n",
    "println(s\"Result: ${result.mkString(\", \")}\")\n",
    "println(\"âœ“ Computation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Chain multiple operations\n",
    "println(\"ðŸ”— Operation Chaining:\")\n",
    "\n",
    "val chainResult = numbersRDD\n",
    "  .filter(_ % 2 == 0)        // Keep even numbers\n",
    "  .map(_ * _ * _)            // Cube them\n",
    "  .filter(_ > 100)           // Keep > 100\n",
    "  .map(math.sqrt(_).toInt)   // Take square root\n",
    "  .collect()\n",
    "\n",
    "println(s\"Chained operations result: ${chainResult.mkString(\", \")}\")\n",
    "println(\"\\nðŸ’¡ Spark optimizes the entire pipeline before execution!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ Word Count Application\n",
    "\n",
    "**The \"Hello World\" of big data - counting words in text.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Sample text data (simulating multiple documents)\n",
    "val textData = Seq(\n",
    "  \"Hello Spark World Hello\",\n",
    "  \"Spark is awesome for big data\",\n",
    "  \"Hello world of distributed computing\",\n",
    "  \"Spark makes big data processing easy\",\n",
    "  \"Hello Spark and Scala world\"\n",
    ")\n",
    "\n",
    "println(\"ðŸ“„ Sample Text Data:\")\n",
    "textData.zipWithIndex.foreach { case (text, idx) =>\n",
    "  println(f\"${idx + 1}. $text\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Classic word count implementation\n",
    "println(\"ðŸ“Š Word Count Implementation:\")\n",
    "\n",
    "val wordCountRDD = sc.parallelize(textData)\n",
    "  .flatMap(line => line.toLowerCase.split(\"\\\\s+\"))  // Split into words\n",
    "  .filter(_.nonEmpty)                                // Remove empty strings\n",
    "  .map(word => (word, 1))                           // Create (word, 1) pairs\n",
    "  .reduceByKey(_ + _)                               // Sum counts by word\n",
    "  .sortBy(_._2, ascending = false)                  // Sort by frequency\n",
    "\n",
    "println(\"Word frequencies:\")\n",
    "println(\"=\" * 30)\n",
    "wordCountRDD.collect().foreach { case (word, count) =>\n",
    "  println(f\"  $word%-12s: $count\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Word count with additional analytics\n",
    "println(\"ðŸ“ˆ Advanced Word Count Analytics:\")\n",
    "\n",
    "val wordStats = wordCountRDD.map { case (word, count) =>\n",
    "  (word, count, word.length)\n",
    "}\n",
    "\n",
    "// Most frequent words\n",
    "val topWords = wordCountRDD.take(5)\n",
    "println(\"\\nðŸ† Top 5 most frequent words:\")\n",
    "topWords.foreach { case (word, count) =>\n",
    "  println(f\"  '$word' appears $count times\")\n",
    "}\n",
    "\n",
    "// Words with frequency > 1\n",
    "val repeatedWords = wordCountRDD.filter(_._2 > 1).collect()\n",
    "println(s\"\\nðŸ”„ Words appearing more than once: ${repeatedWords.length}\")\n",
    "repeatedWords.foreach { case (word, count) =>\n",
    "  println(f\"  '$word': $count times\")\n",
    "}\n",
    "\n",
    "// Total statistics\n",
    "val totalWords = wordCountRDD.map(_._2).sum()\n",
    "val uniqueWords = wordCountRDD.count()\n",
    "val avgFrequency = totalWords.toDouble / uniqueWords\n",
    "\n",
    "println(f\"\\nðŸ“Š Statistics:\")\n",
    "println(f\"  Total words: $totalWords\")\n",
    "println(f\"  Unique words: $uniqueWords\")\n",
    "println(f\"  Average frequency: $avgFrequency%.2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Monitoring & Debugging\n",
    "\n",
    "**Spark provides excellent tools for monitoring and debugging applications.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Application information\n",
    "println(\"ðŸ“Š Application Information:\")\n",
    "println(s\"Application Name: ${spark.sparkContext.appName}\")\n",
    "println(s\"Application ID: ${spark.sparkContext.applicationId}\")\n",
    "println(s\"Spark Version: ${spark.version}\")\n",
    "println(s\"Scala Version: ${scala.util.Properties.versionString}\")\n",
    "println(s\"Master URL: ${spark.sparkContext.master}\")\n",
    "println(s\"Default Parallelism: ${spark.sparkContext.defaultParallelism}\")\n",
    "println(s\"Available Cores: ${Runtime.getRuntime.availableProcessors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// RDD lineage and debugging\n",
    "println(\"ðŸ”— RDD Lineage (for debugging):\")\n",
    "\n",
    "val debugRDD = sc.parallelize(1 to 10)\n",
    "  .filter(_ > 5)\n",
    "  .map(_ * 2)\n",
    "  .filter(_ < 18)\n",
    "\n",
    "println(\"RDD Debug Info:\")\n",
    "println(s\"  Class: ${debugRDD.getClass.getSimpleName}\")\n",
    "println(s\"  Partitions: ${debugRDD.getNumPartitions}\")\n",
    "println(s\"  Dependencies: ${debugRDD.dependencies.length}\")\n",
    "\n",
    "// Show actual lineage\n",
    "println(\"\\nRDD Lineage:\")\n",
    "println(debugRDD.toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Performance monitoring\n",
    "println(\"âš¡ Performance Monitoring:\")\n",
    "\n",
    "val startTime = System.nanoTime()\n",
    "\n",
    "// Perform some operations\n",
    "val performanceTest = sc.parallelize(1 to 10000)\n",
    "  .filter(_ % 2 == 0)\n",
    "  .map(_ * _)\n",
    "  .sum()\n",
    "\n",
    "val endTime = System.nanoTime()\n",
    "val durationMs = (endTime - startTime) / 1e6\n",
    "\n",
    "println(f\"Performance test completed in $durationMs%.2f ms\")\n",
    "println(f\"Result: $performanceTest\")\n",
    "\n",
    "// Memory information\n",
    "val runtime = Runtime.getRuntime\n",
    "val mb = 1024 * 1024\n",
    "println(f\"\\nðŸ§  Memory Usage:\")\n",
    "println(f\"  Total: ${runtime.totalMemory() / mb} MB\")\n",
    "println(f\"  Free: ${runtime.freeMemory() / mb} MB\")\n",
    "println(f\"  Used: ${(runtime.totalMemory() - runtime.freeMemory()) / mb} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ† Exercises\n",
    "\n",
    "**Test your understanding with these hands-on exercises.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Exercise 1: Temperature Analysis\n",
    "// Given a list of temperatures, calculate:\n",
    "// - Average temperature\n",
    "// - Number of days above average\n",
    "// - Hottest and coldest days\n",
    "\n",
    "val temperatures = Seq(22.5, 25.0, 19.8, 28.3, 24.7, 21.1, 26.9, 23.4)\n",
    "\n",
    "// FIXME: Implement temperature analysis\n",
    "val avgTemp = ???  // Calculate average\n",
    "val aboveAvg = ??? // Count days above average\n",
    "val hottest = ???  // Find hottest temperature\n",
    "val coldest = ???  // Find coldest temperature\n",
    "\n",
    "println(\"ðŸŒ¡ï¸ Temperature Analysis:\")\n",
    "println(f\"Average temperature: $avgTemp%.1fÂ°C\")\n",
    "println(f\"Days above average: $aboveAvg\")\n",
    "println(f\"Hottest day: $hottest%.1fÂ°C\")\n",
    "println(f\"Coldest day: $coldest%.1fÂ°C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Exercise 2: Text Analytics\n",
    "// Analyze the text data for:\n",
    "// - Total number of words\n",
    "// - Average words per line\n",
    "// - Most common starting word\n",
    "// - Lines containing \"Spark\"\n",
    "\n",
    "// FIXME: Implement text analytics\n",
    "val totalWords = ???     // Total word count\n",
    "val avgWordsPerLine = ??? // Average words per line\n",
    "val sparkLines = ???     // Lines containing \"Spark\"\n",
    "\n",
    "println(\"ðŸ“ Text Analytics:\")\n",
    "println(s\"Total words: $totalWords\")\n",
    "println(f\"Average words per line: $avgWordsPerLine%.1f\")\n",
    "println(s\"Lines containing 'Spark': ${sparkLines.length}\")\n",
    "sparkLines.foreach(line => println(s\"  $line\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Exercise 3: Number Crunching\n",
    "// Work with numbers 1-100:\n",
    "// - Sum of all numbers\n",
    "// - Count of even numbers\n",
    "// - Product of numbers divisible by 7\n",
    "// - Numbers that are both even and divisible by 3\n",
    "\n",
    "val numbers = 1 to 100\n",
    "\n",
    "// FIXME: Implement number crunching\n",
    "val sumAll = ???           // Sum of all numbers\n",
    "val evenCount = ???        // Count of even numbers\n",
    "val sevenProduct = ???     // Product of numbers divisible by 7\n",
    "val evenAndDivBy3 = ???    // Numbers that are even AND divisible by 3\n",
    "\n",
    "println(\"ðŸ”¢ Number Crunching (1-100):\")\n",
    "println(s\"Sum of all numbers: $sumAll\")\n",
    "println(s\"Even numbers count: $evenCount\")\n",
    "println(s\"Product of multiples of 7: $sevenProduct\")\n",
    "println(s\"Numbers even AND divisible by 3: ${evenAndDivBy3.mkString(\", \")}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ›‘ Cleanup\n",
    "\n",
    "**Always stop your SparkSession to free resources!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Stop SparkSession\n",
    "spark.stop()\n",
    "println(\"ðŸ›‘ Spark Application Stopped\")\n",
    "println(\"âœ… All resources cleaned up!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š What Next?\n",
    "\n",
    "**ðŸŽ‰ Congratulations!** You've completed your first Spark application!\n",
    "\n",
    "**You've learned:**\n",
    "- âœ… Creating and configuring SparkSession\n",
    "- âœ… Working with RDDs (Resilient Distributed Datasets)\n",
    "- âœ… Understanding lazy evaluation and transformations vs actions\n",
    "- âœ… Building word count applications\n",
    "- âœ… Monitoring Spark applications\n",
    "- âœ… Basic performance considerations\n",
    "\n",
    "**Key Concepts Mastered:**\n",
    "- **RDD**: Spark's core distributed data abstraction\n",
    "- **Transformations**: Lazy operations (map, filter, flatMap)\n",
    "- **Actions**: Eager operations that trigger computation (collect, count, sum)\n",
    "- **Lazy Evaluation**: Operations are not executed until an action is called\n",
    "- **Distributed Computing**: Data is automatically partitioned across cores/nodes\n",
    "\n",
    "**Next Steps:**\n",
    "1. Complete all exercises with your own implementations\n",
    "2. Experiment with different RDD operations\n",
    "3. Move to **02: RDD Mastery** for advanced transformations\n",
    "4. Check out the **Spark UI** at http://localhost:4040 (if running locally)\n",
    "\n",
    "**Help & Resources:**\n",
    "- Stuck on exercises? Check the solutions in the `solutions/` directory\n",
    "- Need help? Check the [official Spark documentation](https://spark.apache.org/docs/latest/)\n",
    "- More examples? Look at the `examples/` directory\n",
    "\n",
    "---\n",
    "\n",
    "**Remember:** *\"The best way to learn Spark is by doing Spark things!\"* âš¡"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
