{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”„ RDD Operations & Transformations\n",
    "\n",
    "**Phase 2: RDD Mastery - Core Operations & Transformations**\n",
    "\n",
    "**Prerequisites**: [01_fundamentals/FirstSparkApp.ipynb](../01_fundamentals/FirstSparkApp.ipynb)\n",
    "\n",
    "**Estimated time**: 45-60 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Goals\n",
    "\n",
    "By the end of this notebook, you'll be able to:\n",
    "- âœ… Master essential RDD transformations (map, filter, flatMap)\n",
    "- âœ… Understand lazy evaluation and execution flow\n",
    "- âœ… Perform aggregation operations (reduce, fold, aggregate)\n",
    "- âœ… Use key-value RDD operations (groupByKey, reduceByKey)\n",
    "- âœ… Apply set operations and joins\n",
    "- âœ… Optimize RDD operations for performance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Table of Contents\n",
    "\n",
    "1. [RDD Transformations](#transformations)\n",
    "2. [Aggregation Operations](#aggregation)\n",
    "3. [Key-Value Operations](#keyvalue)\n",
    "4. [Set Operations](#setops)\n",
    "5. [Performance Patterns](#performance)\n",
    "6. [Exercises](#exercises)\n",
    "7. [Next Steps](#next)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ Setup\n",
    "\n",
    "**Initialize Spark Session and sample data for practice.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Import Spark libraries\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create SparkSession\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"RDD Operations Practice\")\n",
    "  .master(\"local[*]\")\n",
    "  .getOrCreate()\n",
    "\n",
    "val sc = spark.sparkContext\n",
    "\n",
    "println(\"ðŸš€ RDD Operations Practice Session Started\")\n",
    "println(s\"Available cores: ${sc.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Sample datasets for practice\n",
    "val numbers = sc.parallelize(Seq(1, 2, 3, 4, 5, 6, 7, 8, 9, 10))\n",
    "val words = sc.parallelize(Seq(\"hello\", \"world\", \"spark\", \"scala\", \"big\", \"data\"))\n",
    "val sales = sc.parallelize(Seq(\n",
    "  (\"Alice\", 100.0),\n",
    "  (\"Bob\", 150.0),\n",
    "  (\"Alice\", 200.0),\n",
    "  (\"Charlie\", 75.0),\n",
    "  (\"Bob\", 120.0),\n",
    "  (\"Alice\", 50.0)\n",
    "))\n",
    "\n",
    "println(\"ðŸ“Š Sample datasets created:\")\n",
    "println(s\"Numbers: ${numbers.collect().mkString(\"[\", \", \", \"]\")}\")\n",
    "println(s\"Words: ${words.collect().mkString(\"[\", \", \", \"]\")}\")\n",
    "println(s\"Sales: ${sales.collect().mkString(\"[\", \", \", \"]\")}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ Essential Transformations\n",
    "\n",
    "**Transformations create new RDDs from existing ones (lazy operations).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// map: Apply function to each element\n",
    "println(\"ðŸ—ºï¸ map Transformation:\")\n",
    "val doubled = numbers.map(_ * 2)\n",
    "println(s\"Original: ${numbers.collect().mkString(\", \")}\")\n",
    "println(s\"Doubled: ${doubled.collect().mkString(\", \")}\")\n",
    "\n",
    "val upperWords = words.map(_.toUpperCase)\n",
    "println(s\"Words upper: ${upperWords.collect().mkString(\", \")}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// filter: Keep elements matching predicate\n",
    "println(\"\\nðŸ” filter Transformation:\")\n",
    "val evens = numbers.filter(_ % 2 == 0)\n",
    "println(s\"Even numbers: ${evens.collect().mkString(\", \")}\")\n",
    "\n",
    "val longWords = words.filter(_.length > 4)\n",
    "println(s\"Long words (>4 chars): ${longWords.collect().mkString(\", \")}\")\n",
    "\n",
    "val highSales = sales.filter(_._2 > 100)\n",
    "println(s\"High sales (>100): ${highSales.collect().mkString(\", \")}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// flatMap: Apply function returning sequence, then flatten\n",
    "println(\"\\nðŸ“„ flatMap Transformation:\")\n",
    "val wordsFlat = words.flatMap(_.toCharArray)\n",
    "println(s\"Word characters: ${wordsFlat.collect().mkString(\", \")}\")\n",
    "\n",
    "val sentences = sc.parallelize(Seq(\"Hello world\", \"Spark is awesome\"))\n",
    "val wordsFromSentences = sentences.flatMap(_.split(\" \"))\n",
    "println(s\"Words from sentences: ${wordsFromSentences.collect().mkString(\", \")}\")\n",
    "\n",
    "val numbersAndSquares = numbers.flatMap(n => Seq(n, n*n))\n",
    "println(s\"Numbers and squares: ${numbersAndSquares.collect().mkString(\", \")}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// distinct: Remove duplicates\n",
    "println(\"\\nðŸŽ¯ distinct Transformation:\")\n",
    "val duplicates = sc.parallelize(Seq(1, 2, 2, 3, 3, 3, 4, 5, 5))\n",
    "val unique = duplicates.distinct()\n",
    "println(s\"With duplicates: ${duplicates.collect().mkString(\", \")}\")\n",
    "println(s\"Distinct: ${unique.collect().mkString(\", \")}\")\n",
    "\n",
    "val duplicateWords = sc.parallelize(Seq(\"hello\", \"world\", \"hello\", \"spark\", \"world\"))\n",
    "val uniqueWords = duplicateWords.distinct()\n",
    "println(s\"Words with duplicates: ${duplicateWords.collect().mkString(\", \")}\")\n",
    "println(s\"Unique words: ${uniqueWords.collect().mkString(\", \")}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// union, intersection, subtract\n",
    "println(\"\\nðŸ”— Set Operations:\")\n",
    "val setA = sc.parallelize(Seq(1, 2, 3, 4, 5))\n",
    "val setB = sc.parallelize(Seq(4, 5, 6, 7, 8))\n",
    "val setC = sc.parallelize(Seq(1, 2, 6, 7, 9))\n",
    "\n",
    "println(s\"Set A: ${setA.collect().sorted.mkString(\", \")}\")\n",
    "println(s\"Set B: ${setB.collect().sorted.mkString(\", \")}\")\n",
    "println(s\"Set C: ${setC.collect().sorted.mkString(\", \")}\")\n",
    "\n",
    "val unionAB = setA.union(setB)\n",
    "println(s\"A âˆª B: ${unionAB.collect().sorted.mkString(\", \")}\")\n",
    "\n",
    "val intersectionAB = setA.intersection(setB)\n",
    "println(s\"A âˆ© B: ${intersectionAB.collect().sorted.mkString(\", \")}\")\n",
    "\n",
    "val subtractAC = setA.subtract(setC)\n",
    "println(s\"A - C: ${subtractAC.collect().sorted.mkString(\", \")}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Aggregation Operations\n",
    "\n",
    "**Actions that combine RDD elements into single values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// reduce: Combine elements with associative function\n",
    "println(\"ðŸ”¢ reduce Action:\")\n",
    "val sum = numbers.reduce(_ + _)\n",
    "println(s\"Sum of numbers: $sum\")\n",
    "\n",
    "val product = numbers.filter(_ <= 5).reduce(_ * _)\n",
    "println(s\"Product of numbers â‰¤ 5: $product\")\n",
    "\n",
    "val concatenated = words.reduce(_ + \" \" + _)\n",
    "println(s\"Concatenated words: $concatenated\")\n",
    "\n",
    "// Note: reduce fails on empty RDD\n",
    "// val emptySum = sc.parallelize(Seq[Int]()).reduce(_ + _)  // Throws exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// fold: Similar to reduce but with initial value (safer)\n",
    "println(\"\\nðŸ“¦ fold Action:\")\n",
    "val sumWithFold = numbers.fold(0)(_ + _)\n",
    "println(s\"Sum with fold: $sumWithFold\")\n",
    "\n",
    "val emptySum = sc.parallelize(Seq[Int]()).fold(0)(_ + _)\n",
    "println(s\"Empty RDD sum: $emptySum\")\n",
    "\n",
    "val wordLengths = words.fold(0)(_ + _.length)\n",
    "println(s\"Total characters in words: $wordLengths\")\n",
    "\n",
    "val maxLength = words.fold(0)((acc, word) => math.max(acc, word.length))\n",
    "println(s\"Longest word length: $maxLength\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// aggregate: Most general aggregation (initial value + combine functions)\n",
    "println(\"\\nðŸ”§ aggregate Action:\")\n",
    "\n",
    "// Count and sum in one operation\n",
    "val (count, total) = numbers.aggregate((0, 0))(\n",
    "  // Within partition: (count, sum) + element\n",
    "  (acc, num) => (acc._1 + 1, acc._2 + num),\n",
    "  // Between partitions: combine results\n",
    "  (acc1, acc2) => (acc1._1 + acc2._1, acc1._2 + acc2._2)\n",
    ")\n",
    "println(f\"Count: $count, Total: $total, Average: ${total.toDouble/count}%.2f\")\n",
    "\n",
    "// Statistics in one pass\n",
    "case class Stats(count: Int, sum: Int, min: Int, max: Int)\n",
    "val stats = numbers.aggregate(Stats(0, 0, Int.MaxValue, Int.MinValue))(\n",
    "  (acc, num) => Stats(\n",
    "    acc.count + 1,\n",
    "    acc.sum + num,\n",
    "    math.min(acc.min, num),\n",
    "    math.max(acc.max, num)\n",
    "  ),\n",
    "  (acc1, acc2) => Stats(\n",
    "    acc1.count + acc2.count,\n",
    "    acc1.sum + acc2.sum,\n",
    "    math.min(acc1.min, acc2.min),\n",
    "    math.max(acc1.max, acc2.max)\n",
    "  )\n",
    ")\n",
    "println(f\"Stats - Count: ${stats.count}, Sum: ${stats.sum}, Min: ${stats.min}, Max: ${stats.max}, Avg: ${stats.sum.toDouble/stats.count}%.2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”‘ Key-Value Operations\n",
    "\n",
    "**Operations on RDDs containing key-value pairs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// groupByKey: Group values by key\n",
    "println(\"ðŸ”‘ groupByKey Operation:\")\n",
    "val groupedSales = sales.groupByKey()\n",
    "println(\"Sales grouped by person:\")\n",
    "groupedSales.collect().foreach { case (person, amounts) =>\n",
    "  println(f\"  $person%-8s: ${amounts.mkString(\"[\", \", \", \"]\")}\")\n",
    "}\n",
    "\n",
    "// Warning: groupByKey can cause memory issues!\n",
    "// Better to use reduceByKey when possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// reduceByKey: Combine values with same key (more efficient)\n",
    "println(\"\\nâš¡ reduceByKey Operation:\")\n",
    "val totalSales = sales.reduceByKey(_ + _)\n",
    "println(\"Total sales by person:\")\n",
    "totalSales.collect().foreach { case (person, total) =>\n",
    "  println(f\"  $person%-8s: $$${total}%.2f\")\n",
    "}\n",
    "\n",
    "// Count words with reduceByKey\n",
    "val wordCounts = words.map(word => (word, 1)).reduceByKey(_ + _)\n",
    "println(\"\\nWord counts:\")\n",
    "wordCounts.collect().foreach { case (word, count) =>\n",
    "  println(f\"  $word%-8s: $count\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Other key-value operations\n",
    "println(\"\\nðŸ”„ Other Key-Value Operations:\")\n",
    "\n",
    "// sortByKey\n",
    "val sortedSales = totalSales.sortByKey()\n",
    "println(\"Sales sorted by name:\")\n",
    "sortedSales.collect().foreach { case (person, total) =>\n",
    "  println(f\"  $person%-8s: $$${total}%.2f\")\n",
    "}\n",
    "\n",
    "// mapValues (transform values only)\n",
    "val formattedSales = totalSales.mapValues(amount => f\"$$$amount%.2f\")\n",
    "println(\"\\nFormatted sales:\")\n",
    "formattedSales.collect().foreach { case (person, formatted) =>\n",
    "  println(f\"  $person%-8s: $formatted\")\n",
    "}\n",
    "\n",
    "// keys and values\n",
    "val people = totalSales.keys.distinct()\n",
    "val amounts = totalSales.values\n",
    "println(s\"\\nPeople: ${people.collect().mkString(\", \")}\")\n",
    "println(s\"Amounts: ${amounts.collect().mkString(\", \")}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// join operations\n",
    "println(\"\\nðŸ¤ Join Operations:\")\n",
    "\n",
    "val salesRDD = sc.parallelize(Seq(\n",
    "  (\"Alice\", 100.0),\n",
    "  (\"Bob\", 150.0),\n",
    "  (\"Charlie\", 75.0)\n",
    "))\n",
    "\n",
    "val regionsRDD = sc.parallelize(Seq(\n",
    "  (\"Alice\", \"North\"),\n",
    "  (\"Bob\", \"South\"),\n",
    "  (\"David\", \"East\")\n",
    "))\n",
    "\n",
    "val innerJoin = salesRDD.join(regionsRDD)\n",
    "println(\"Inner join (sales with regions):\")\n",
    "innerJoin.collect().foreach { case (person, (sales, region)) =>\n",
    "  println(f\"  $person%-8s: $$$sales%.2f ($region)\")\n",
    "}\n",
    "\n",
    "val leftJoin = salesRDD.leftOuterJoin(regionsRDD)\n",
    "println(\"\\nLeft join (all sales, optional regions):\")\n",
    "leftJoin.collect().foreach { case (person, (sales, regionOpt)) =>\n",
    "  val region = regionOpt.getOrElse(\"Unknown\")\n",
    "  println(f\"  $person%-8s: $$$sales%.2f ($region)\")\n",
    "}\n",
    "\n",
    "val rightJoin = salesRDD.rightOuterJoin(regionsRDD)\n",
    "println(\"\\nRight join (all regions, optional sales):\")\n",
    "rightJoin.collect().foreach { case (person, (salesOpt, region)) =>\n",
    "  val sales = salesOpt.map(s => f\"$$$s%.2f\").getOrElse(\"No sales\")\n",
    "  println(f\"  $person%-8s: $sales ($region)\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ Performance Patterns\n",
    "\n",
    "**Understanding when to use each operation for optimal performance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Performance comparison: groupByKey vs reduceByKey\n",
    "println(\"âš¡ Performance Comparison:\")\n",
    "\n",
    "// Create larger dataset for meaningful comparison\n",
    "val largeSales = sc.parallelize((1 to 1000).map(i => (s\"Person${i % 10}\", i.toDouble)))\n",
    "\n",
    "println(\"Dataset size: \" + largeSales.count())\n",
    "\n",
    "// Method 1: groupByKey + mapValues (inefficient)\n",
    "val start1 = System.nanoTime()\n",
    "val result1 = largeSales.groupByKey().mapValues(_.sum).collect()\n",
    "val time1 = (System.nanoTime() - start1) / 1e6\n",
    "\n",
    "// Method 2: reduceByKey (efficient)\n",
    "val start2 = System.nanoTime()\n",
    "val result2 = largeSales.reduceByKey(_ + _).collect()\n",
    "val time2 = (System.nanoTime() - start2) / 1e6\n",
    "\n",
    "println(f\"groupByKey + mapValues: ${time1}%.2f ms\")\n",
    "println(f\"reduceByKey: ${time2}%.2f ms\")\n",
    "println(f\"reduceByKey is ${time1/time2}%.1f x faster\")\n",
    "\n",
    "// Results should be the same\n",
    "println(\"Results match: \" + result1.toSet.equals(result2.toSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Understanding shuffles and data movement\n",
    "println(\"\\nðŸ”€ Understanding Shuffles:\")\n",
    "\n",
    "// Operations that cause shuffles (data movement between nodes)\n",
    "val data = sc.parallelize(Seq(1, 2, 3, 4, 5, 6, 7, 8, 9, 10))\n",
    "\n",
    "println(\"Operations causing shuffles:\")\n",
    "println(\"- groupByKey: Moves all values for same key to one node\")\n",
    "println(\"- reduceByKey: Combines locally first, then moves\")\n",
    "println(\"- join: Requires keys to be on same node\")\n",
    "println(\"- sortByKey: Requires global ordering\")\n",
    "println(\"- distinct: Requires deduplication across nodes\")\n",
    "\n",
    "// Demonstrate reduceByKey vs groupByKey shuffle difference\n",
    "val kvData = sc.parallelize(Seq(\n",
    "  (\"A\", 1), (\"B\", 2), (\"A\", 3), (\"B\", 4), (\"A\", 5)\n",
    "))\n",
    "\n",
    "// reduceByKey: Combine locally before shuffle\n",
    "val reduceResult = kvData.reduceByKey(_ + _)\n",
    "println(s\"\\nreduceByKey result: ${reduceResult.collect().mkString(\", \")}\")\n",
    "\n",
    "// groupByKey: Shuffle all values\n",
    "val groupResult = kvData.groupByKey().mapValues(_.sum)\n",
    "println(s\"groupByKey result: ${groupResult.collect().mkString(\", \")}\")\n",
    "\n",
    "println(\"\\nBoth give same result, but reduceByKey is more efficient!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ† Practice Exercises\n",
    "\n",
    "**Apply what you've learned with these hands-on exercises.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Exercise 1: Text Analysis\n",
    "// Given a list of sentences, compute:\n",
    "// - Total word count\n",
    "// - Average words per sentence\n",
    "// - Most frequent word\n",
    "// - Words appearing in multiple sentences\n",
    "\n",
    "val sentences = Seq(\n",
    "  \"Spark is a powerful big data framework\",\n",
    "  \"Scala and Spark work great together\",\n",
    "  \"Big data processing with Spark is efficient\",\n",
    "  \"Spark provides excellent performance\",\n",
    "  \"Learning Spark and Scala is rewarding\"\n",
    ")\n",
    "\n",
    "// FIXME: Implement text analysis\n",
    "val sentencesRDD = sc.parallelize(sentences)\n",
    "\n",
    "// Total word count\n",
    "val totalWords = ???  // Hint: flatMap + count\n",
    "\n",
    "// Average words per sentence\n",
    "val avgWordsPerSentence = ???  // Hint: map + reduce\n",
    "\n",
    "// Most frequent word\n",
    "val mostFrequentWord = ???  // Hint: flatMap + map + reduceByKey + sort\n",
    "\n",
    "// Words in multiple sentences\n",
    "val multiSentenceWords = ???  // Hint: flatMap + distinct + groupByKey + filter\n",
    "\n",
    "println(\"ðŸ“ Text Analysis Results:\")\n",
    "println(s\"Total words: $totalWords\")\n",
    "println(f\"Average words per sentence: $avgWordsPerSentence%.1f\")\n",
    "println(s\"Most frequent word: $mostFrequentWord\")\n",
    "println(s\"Words in multiple sentences: ${multiSentenceWords.mkString(\", \")}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Exercise 2: Sales Analytics\n",
    "// Given sales data, compute:\n",
    "// - Total revenue by product\n",
    "// - Top 3 products by revenue\n",
    "// - Products with revenue > 500\n",
    "// - Average revenue per product\n",
    "\n",
    "val salesData = Seq(\n",
    "  (\"Laptop\", 1200.0, 3),\n",
    "  (\"Mouse\", 25.0, 10),\n",
    "  (\"Keyboard\", 75.0, 5),\n",
    "  (\"Monitor\", 300.0, 8),\n",
    "  (\"Laptop\", 1200.0, 2),\n",
    "  (\"Mouse\", 25.0, 15),\n",
    "  (\"Headphones\", 150.0, 6),\n",
    "  (\"Monitor\", 300.0, 4)\n",
    ")\n",
    "\n",
    "// FIXME: Implement sales analytics\n",
    "val salesRDD = sc.parallelize(salesData)\n",
    "\n",
    "// Calculate revenue (price * quantity) and group by product\n",
    "val revenueByProduct = salesRDD.map { case (product, price, qty) =>\n",
    "  (product, price * qty)\n",
    "}.reduceByKey(_ + _)\n",
    "\n",
    "// Total revenue by product\n",
    "val totalRevenueByProduct = ???  // Hint: Use revenueByProduct\n",
    "\n",
    "// Top 3 products by revenue\n",
    "val top3Products = ???  // Hint: sortBy + take\n",
    "\n",
    "// Products with revenue > 500\n",
    "val highRevenueProducts = ???  // Hint: filter\n",
    "\n",
    "// Average revenue per product\n",
    "val avgRevenuePerProduct = ???  // Hint: Use revenueByProduct + aggregate\n",
    "\n",
    "println(\"ðŸ’° Sales Analytics Results:\")\n",
    "println(\"Total revenue by product:\")\n",
    "totalRevenueByProduct.foreach { case (product, revenue) =>\n",
    "  println(f\"  $product%-12s: $$$revenue%.2f\")\n",
    "}\n",
    "println(s\"\\nTop 3 products: ${top3Products.mkString(\", \")}\")\n",
    "println(s\"High revenue products (>500): ${highRevenueProducts.mkString(\", \")}\")\n",
    "println(f\"Average revenue per product: $$$avgRevenuePerProduct%.2f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Exercise 3: Performance Optimization\n",
    "// Compare different approaches for the same computation\n",
    "// Goal: Find sum of squares for numbers 1-1000\n",
    "\n",
    "val largeNumbers = 1 to 1000\n",
    "\n",
    "// FIXME: Implement and compare different approaches\n",
    "val numbersRDD = sc.parallelize(largeNumbers)\n",
    "\n",
    "// Approach 1: map + reduce\n",
    "val start1 = System.nanoTime()\n",
    "val result1 = ???  // map(_ * _) then reduce(_ + _)\n",
    "val time1 = (System.nanoTime() - start1) / 1e6\n",
    "\n",
    "// Approach 2: aggregate\n",
    "val start2 = System.nanoTime()\n",
    "val result2 = ???  // Use aggregate with (0, 0) accumulator\n",
    "val time2 = (System.nanoTime() - start2) / 1e6\n",
    "\n",
    "// Approach 3: fold\n",
    "val start3 = System.nanoTime()\n",
    "val result3 = ???  // Use fold with 0\n",
    "val time3 = (System.nanoTime() - start3) / 1e6\n",
    "\n",
    "println(\"âš¡ Performance Comparison - Sum of Squares (1-1000):\")\n",
    "println(f\"map + reduce:     $time1%.2f ms (result: $result1)\")\n",
    "println(f\"aggregate:         $time2%.2f ms (result: $result2)\")\n",
    "println(f\"fold:             $time3%.2f ms (result: $result3)\")\n",
    "\n",
    "val fastest = List((time1, \"map+reduce\"), (time2, \"aggregate\"), (time3, \"fold\")).minBy(_._1)\n",
    "println(s\"\\nFastest approach: ${fastest._2}\")\n",
    "println(\"All results match: \" + (result1 == result2 && result2 == result3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ›‘ Cleanup\n",
    "\n",
    "**Always stop your SparkSession to free resources!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Stop SparkSession\n",
    "spark.stop()\n",
    "println(\"ðŸ›‘ Spark Session Stopped\")\n",
    "println(\"âœ… All resources cleaned up!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š What Next?\n",
    "\n",
    "**ðŸŽ‰ Congratulations!** You've mastered essential RDD operations!\n",
    "\n",
    "**You've learned:**\n",
    "- âœ… Core transformations: map, filter, flatMap, distinct\n",
    "- âœ… Aggregation operations: reduce, fold, aggregate\n",
    "- âœ… Key-value operations: groupByKey, reduceByKey, joins\n",
    "- âœ… Set operations: union, intersection, subtract\n",
    "- âœ… Performance patterns: choosing the right operation\n",
    "- âœ… Lazy evaluation and execution optimization\n",
    "\n",
    "**Key Concepts Mastered:**\n",
    "- **Transformations vs Actions**: Lazy vs eager execution\n",
    "- **Shuffle Operations**: Understanding data movement costs\n",
    "- **Key-Value RDDs**: Efficient grouped operations\n",
    "- **Performance Trade-offs**: When to use each operation\n",
    "- **Distributed Computing**: How operations execute across nodes\n",
    "\n",
    "**Next Steps:**\n",
    "1. Complete all exercises with your own implementations\n",
    "2. Experiment with different RDD operations on various datasets\n",
    "3. Move to **03: DataFrame Operations** for structured data processing\n",
    "4. Practice identifying which operations to use for different problems\n",
    "\n",
    "**Performance Tip:** Always prefer `reduceByKey` over `groupByKey` + transformation for better performance!\n",
    "\n",
    "**Remember:** RDDs are the foundation - everything in Spark builds on these core operations! âš¡"
   ]
  }
  ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
