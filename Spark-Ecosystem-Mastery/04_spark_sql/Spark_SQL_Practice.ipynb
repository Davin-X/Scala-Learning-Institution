{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç Spark SQL & DataFrame SQL Operations\n",
    "\n",
    "**Phase 4: SQL Mastery - Querying Big Data with SQL**\n",
    "\n",
    "**Prerequisites**: [03_dataframe_mastery/DataFrame_Operations_Practice.ipynb](../03_dataframe_mastery/DataFrame_Operations_Practice.ipynb)\n",
    "\n",
    "**Estimated time**: 45-60 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Goals\n",
    "\n",
    "By the end of this notebook, you'll be able to:\n",
    "- ‚úÖ Create and manage SQL tables and views\n",
    "- ‚úÖ Write complex SQL queries with joins and aggregations\n",
    "- ‚úÖ Use window functions in SQL\n",
    "- ‚úÖ Optimize query performance with Catalyst\n",
    "- ‚úÖ Handle different data formats (JSON, Parquet, CSV)\n",
    "- ‚úÖ Convert between DataFrame API and SQL\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Setup & Data Preparation\n",
    "\n",
    "**Create sample data and register it for SQL queries.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Import Spark SQL\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create SparkSession with Hive support\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"Spark SQL Practice\")\n",
    "  .master(\"local[*]\")\n",
    "  .enableHiveSupport()  // Enable Hive features\n",
    "  .getOrCreate()\n",
    "\n",
    "println(\"üöÄ Spark SQL Practice Session Started\")\n",
    "println(s\"Spark Version: ${spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Creating Tables & Views\n",
    "\n",
    "**Register DataFrames as SQL tables and create views.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create sample datasets\n",
    "val employees = Seq(\n",
    "  (1, \"Alice\", \"Engineering\", 75000, \"2020-01-15\"),\n",
    "  (2, \"Bob\", \"Engineering\", 80000, \"2019-03-22\"),\n",
    "  (3, \"Charlie\", \"Sales\", 65000, \"2021-07-10\"),\n",
    "  (4, \"Diana\", \"Engineering\", 90000, \"2018-11-05\")\n",
    ")\n",
    "\n",
    "// Create DataFrame and register as view\n",
    "val employeesDF = employees.toDF(\"id\", \"name\", \"dept\", \"salary\", \"hire_date\")\n",
    "employeesDF.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "println(\"‚úÖ Employees table registered\")\n",
    "spark.sql(\"SELECT * FROM employees\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Basic SQL Queries\n",
    "\n",
    "**Execute SQL queries on your registered tables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Basic SELECT queries\n",
    "println(\"üîç Basic SQL Queries:\")\n",
    "\n",
    "spark.sql(\"SELECT name, dept, salary FROM employees ORDER BY salary DESC\").show()\n",
    "\n",
    "spark.sql(\"SELECT dept, COUNT(*) as count FROM employees GROUP BY dept\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ù SQL Joins\n",
    "\n",
    "**Combine data from multiple tables using SQL joins.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create departments data\n",
    "val departments = Seq(\n",
    "  (\"Engineering\", \"Technical\"),\n",
    "  (\"Sales\", \"Business\")\n",
    ")\n",
    "val deptDF = departments.toDF(\"dept_name\", \"category\")\n",
    "deptDF.createOrReplaceTempView(\"departments\")\n",
    "\n",
    "// INNER JOIN\n",
    "println(\"ü§ù SQL Joins:\")\n",
    "spark.sql(\"\"\"\n",
    "  SELECT e.name, e.dept, d.category, e.salary\n",
    "  FROM employees e\n",
    "  INNER JOIN departments d ON e.dept = d.dept_name\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÜ Practice Exercises\n",
    "\n",
    "**Write SQL queries to solve real-world analytics problems.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Exercise 1: Employee Analytics\n",
    "// FIXME: Write SQL queries for:\n",
    "// 1. Find highest paid employee\n",
    "// 2. Count employees by department\n",
    "// 3. Calculate average salary\n",
    "\n",
    "println(\"üíº Employee Analytics Exercise:\")\n",
    "\n",
    "// 1. Highest paid employee\n",
    "spark.sql(\"SELECT name, salary FROM employees ORDER BY salary DESC LIMIT 1\").show()\n",
    "\n",
    "// 2. Count by department\n",
    "spark.sql(\"SELECT dept, COUNT(*) as count FROM employees GROUP BY dept\").show()\n",
    "\n",
    "// 3. Average salary\n",
    "spark.sql(\"SELECT AVG(salary) as avg_salary FROM employees\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõë Cleanup\n",
    "\n",
    "**Drop temporary views and stop SparkSession.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Stop SparkSession\n",
    "spark.stop()\n",
    "println(\"üõë Spark SQL Session Stopped\")\n",
    "println(\"‚úÖ All resources cleaned up!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö What Next?\n",
    "\n",
    "**üéâ Congratulations!** You've mastered Spark SQL basics!\n",
    "\n",
    "**You've learned:**\n",
    "- ‚úÖ Creating SQL tables and views\n",
    "- ‚úÖ Basic SQL queries and aggregations\n",
    "- ‚úÖ SQL joins between tables\n",
    "\n",
    "**Next Steps:**\n",
    "1. Complete all exercises with your own SQL implementations\n",
    "2. Move to **05_performance_optimization/** for tuning\n",
    "3. Explore **08_real_world_projects/** for production SQL\n",
    "\n",
    "**Remember:** Spark SQL brings SQL power to big data! ‚ö°"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
