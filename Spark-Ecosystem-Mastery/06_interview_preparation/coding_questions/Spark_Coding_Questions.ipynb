{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Spark Coding Interview Questions\n",
    "\n",
    "**Master the Most Common Spark Coding Interview Questions**\n",
    "\n",
    "**Prerequisites**: All previous modules (01-05)\n",
    "\n",
    "**Estimated time**: 90-120 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Interview Question Categories\n",
    "\n",
    "This notebook covers the most frequently asked Spark coding questions:\n",
    "- ‚úÖ **RDD Operations**: Transformations, actions, key-value operations\n",
    "- ‚úÖ **DataFrame Operations**: Filtering, aggregations, joins, window functions\n",
    "- ‚úÖ **Performance Optimization**: Caching, partitioning, shuffle minimization\n",
    "- ‚úÖ **SQL Queries**: Complex queries, window functions, CTEs\n",
    "- ‚úÖ **Real-World Scenarios**: ETL pipelines, data processing patterns\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Setup\n",
    "\n",
    "**Initialize Spark for interview practice.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Import Spark libraries\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create SparkSession\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"Spark Interview Questions\")\n",
    "  .master(\"local[*]\")\n",
    "  .getOrCreate()\n",
    "\n",
    "val sc = spark.sparkContext\n",
    "\n",
    "println(\"üöÄ Spark Interview Practice Session Started\")\n",
    "println(s\"Spark Version: ${spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¢ Question 1: Word Count (Classic Interview Question)\n",
    "\n",
    "**Implement word count with multiple optimizations.**\n",
    "\n",
    "**Difficulty**: ‚≠ê‚≠ê‚≠ê\n",
    "**Frequency**: Very High\n",
    "\n",
    "**Requirements**:\n",
    "- Count word frequencies in a text\n",
    "- Handle case sensitivity\n",
    "- Remove punctuation\n",
    "- Return top N words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Sample text data\n",
    "val textData = Seq(\n",
    "  \"Spark is a powerful big data framework!\",\n",
    "  \"Scala and Spark work great together.\",\n",
    "  \"Big data processing with Spark is efficient.\",\n",
    "  \"Learning Spark and Scala is rewarding!\",\n",
    "  \"Spark provides excellent performance.\"\n",
    ")\n",
    "\n",
    "// FIXME: Implement optimized word count\n",
    "def wordCount(texts: Seq[String], topN: Int = 5): Array[(String, Int)] = {\n",
    "  // Your implementation here\n",
    "  // 1. Create RDD from texts\n",
    "  // 2. Split into words (remove punctuation)\n",
    "  // 3. Convert to lowercase\n",
    "  // 4. Count frequencies\n",
    "  // 5. Return top N words\n",
    "  \n",
    "  ??? // Replace with your solution\n",
    "}\n",
    "\n",
    "// Test your implementation\n",
    "val result = wordCount(textData, 3)\n",
    "println(\"Top 3 words:\")\n",
    "result.foreach { case (word, count) =>\n",
    "  println(f\"  $word%-10s: $count\")\n",
    "}\n",
    "\n",
    "// Expected: spark: 4, data: 2, is: 2 (or similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Question 2: Employee Salary Analysis\n",
    "\n",
    "**Analyze employee data using DataFrame operations.**\n",
    "\n",
    "**Difficulty**: ‚≠ê‚≠ê‚≠ê‚≠ê\n",
    "**Frequency**: High\n",
    "\n",
    "**Requirements**:\n",
    "- Find highest paid employee per department\n",
    "- Calculate department statistics\n",
    "- Identify employees above average salary\n",
    "- Show salary distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Employee data\n",
    "val employees = Seq(\n",
    "  (1, \"Alice\", \"Engineering\", 75000, \"2020-01-15\"),\n",
    "  (2, \"Bob\", \"Engineering\", 80000, \"2019-03-22\"),\n",
    "  (3, \"Charlie\", \"Sales\", 65000, \"2021-07-10\"),\n",
    "  (4, \"Diana\", \"Engineering\", 90000, \"2018-11-05\"),\n",
    "  (5, \"Eve\", \"Marketing\", 70000, \"2020-09-18\"),\n",
    "  (6, \"Frank\", \"Sales\", 72000, \"2019-12-08\"),\n",
    "  (7, \"Grace\", \"Engineering\", 85000, \"2021-02-28\"),\n",
    "  (8, \"Henry\", \"Marketing\", 68000, \"2020-06-14\")\n",
    ")\n",
    "\n",
    "val employeesDF = employees.toDF(\"id\", \"name\", \"dept\", \"salary\", \"hire_date\")\n",
    "\n",
    "// FIXME: Implement salary analysis\n",
    "\n",
    "// 1. Highest paid employee per department\n",
    "println(\"1. Highest paid employee per department:\")\n",
    "val highestPaid = ??? // Window function with row_number\n",
    "highestPaid.show()\n",
    "\n",
    "// 2. Department statistics\n",
    "println(\"\\n2. Department statistics:\")\n",
    "val deptStats = ??? // Group by with aggregations\n",
    "deptStats.show()\n",
    "\n",
    "// 3. Employees above department average\n",
    "println(\"\\n3. Employees above department average:\")\n",
    "val aboveAvg = ??? // Join with subquery or window function\n",
    "aboveAvg.show()\n",
    "\n",
    "// 4. Salary distribution\n",
    "println(\"\\n4. Salary distribution:\")\n",
    "val salaryDist = ??? // Bucket salaries into ranges\n",
    "salaryDist.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîë Question 3: Key-Value RDD Operations\n",
    "\n",
    "**Implement complex key-value transformations.**\n",
    "\n",
    "**Difficulty**: ‚≠ê‚≠ê‚≠ê‚≠ê\n",
    "**Frequency**: High\n",
    "\n",
    "**Requirements**:\n",
    "- Group records by key\n",
    "- Calculate aggregations per group\n",
    "- Find top N per group\n",
    "- Join with another dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Sales data: (customer_id, product, amount, date)\n",
    "val salesData = Seq(\n",
    "  (1, \"Laptop\", 1200.0, \"2023-01\"),\n",
    "  (1, \"Mouse\", 25.0, \"2023-01\"),\n",
    "  (2, \"Laptop\", 1200.0, \"2023-01\"),\n",
    "  (1, \"Keyboard\", 75.0, \"2023-02\"),\n",
    "  (2, \"Monitor\", 300.0, \"2023-02\"),\n",
    "  (3, \"Laptop\", 1200.0, \"2023-02\"),\n",
    "  (2, \"Mouse\", 25.0, \"2023-03\"),\n",
    "  (3, \"Keyboard\", 75.0, \"2023-03\")\n",
    ")\n",
    "\n",
    "// FIXME: Implement key-value operations\n",
    "val salesRDD = sc.parallelize(salesData)\n",
    "\n",
    "// 1. Total sales per customer\n",
    "println(\"1. Total sales per customer:\")\n",
    "val customerTotals = ??? // reduceByKey\n",
    "customerTotals.collect().foreach(println)\n",
    "\n",
    "// 2. Customer purchase history\n",
    "println(\"\\n2. Customer purchase history:\")\n",
    "val customerHistory = ??? // groupByKey\n",
    "customerHistory.take(2).foreach { case (cust, purchases) =>\n",
    "  println(s\"Customer $cust: ${purchases.mkString(\", \")}\")\n",
    "}\n",
    "\n",
    "// 3. Top product per customer by amount\n",
    "println(\"\\n3. Top product per customer:\")\n",
    "val topProducts = ??? // Complex transformation\n",
    "topProducts.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Question 4: Performance Optimization\n",
    "\n",
    "**Optimize a slow Spark job.**\n",
    "\n",
    "**Difficulty**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n",
    "**Frequency**: Very High\n",
    "\n",
    "**Scenario**: A Spark job is running slowly. Identify and fix performance issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Simulate a slow job\n",
    "val largeData = sc.parallelize(1 to 100000).map(i => (s\"Group${i % 100}\", i))\n",
    "\n",
    "// FIXME: Optimize this slow implementation\n",
    "println(\"Original slow implementation:\")\n",
    "val start1 = System.nanoTime()\n",
    "\n",
    "// Inefficient: groupByKey + mapValues\n",
    "val result1 = largeData\n",
    "  .groupByKey()\n",
    "  .mapValues(_.sum)\n",
    "  .filter(_._2 > 5000000)\n",
    "  .collect()\n",
    "\n",
    "val time1 = (System.nanoTime() - start1) / 1e6\n",
    "println(f\"Original approach: $time1%.2f ms, Results: ${result1.length}\")\n",
    "\n",
    "// FIXME: Implement optimized version\n",
    "println(\"\\nOptimized implementation:\")\n",
    "val start2 = System.nanoTime()\n",
    "\n",
    "// Optimized: reduceByKey + coalesce\n",
    "val result2 = ??? // Your optimized implementation\n",
    "  // Hint: Use reduceByKey, coalesce, cache strategically\n",
    "\n",
    "val time2 = (System.nanoTime() - start2) / 1e6\n",
    "println(f\"Optimized approach: $time2%.2f ms, Results: ${result2.length}\")\n",
    "\n",
    "println(f\"\\nPerformance improvement: ${time1/time2}%.1fx faster\")\n",
    "\n",
    "// Explain your optimizations:\n",
    "println(\"\\nüí° Optimizations applied:\")\n",
    "println(\"   1. replace groupByKey with reduceByKey\")\n",
    "println(\"   2. use coalesce to reduce partitions\")\n",
    "println(\"   3. cache intermediate results if needed\")\n",
    "println(\"   4. minimize shuffles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Question 5: DataFrame Joins & Window Functions\n",
    "\n",
    "**Complex DataFrame operations with joins and analytics.**\n",
    "\n",
    "**Difficulty**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n",
    "**Frequency**: High\n",
    "\n",
    "**Requirements**:\n",
    "- Join multiple DataFrames\n",
    "- Use window functions for rankings\n",
    "- Calculate running totals\n",
    "- Handle complex business logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create multiple related datasets\n",
    "val orders = Seq(\n",
    "  (1001, \"Alice\", \"2023-01-15\", 150.0),\n",
    "  (1002, \"Bob\", \"2023-01-16\", 200.0),\n",
    "  (1003, \"Alice\", \"2023-01-17\", 75.0),\n",
    "  (1004, \"Charlie\", \"2023-01-18\", 300.0),\n",
    "  (1005, \"Bob\", \"2023-01-19\", 125.0)\n",
    ")\n",
    "\n",
    "val customers = Seq(\n",
    "  (\"Alice\", \"Premium\"),\n",
    "  (\"Bob\", \"Standard\"),\n",
    "  (\"Charlie\", \"Premium\"),\n",
    "  (\"Diana\", \"Basic\")\n",
    ")\n",
    "\n",
    "val ordersDF = orders.toDF(\"order_id\", \"customer\", \"order_date\", \"amount\")\n",
    "val customersDF = customers.toDF(\"customer\", \"tier\")\n",
    "\n",
    "// FIXME: Implement complex DataFrame operations\n",
    "\n",
    "// 1. Join orders with customer tiers\n",
    "println(\"1. Orders with customer tiers:\")\n",
    "val ordersWithTiers = ??? // Join DataFrames\n",
    "ordersWithTiers.show()\n",
    "\n",
    "// 2. Customer ranking by total spend\n",
    "println(\"\\n2. Customer ranking by total spend:\")\n",
    "val customerRanking = ??? // Window function with rank\n",
    "customerRanking.show()\n",
    "\n",
    "// 3. Running total per customer\n",
    "println(\"\\n3. Running total per customer:\")\n",
    "val runningTotals = ??? // Window function with sum over order by date\n",
    "runningTotals.show()\n",
    "\n",
    "// 4. Premium customer analysis\n",
    "println(\"\\n4. Premium customer analysis:\")\n",
    "val premiumAnalysis = ??? // Filter + aggregations\n",
    "premiumAnalysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÜ Question 6: ETL Pipeline Design\n",
    "\n",
    "**Design and implement a complete ETL pipeline.**\n",
    "\n",
    "**Difficulty**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n",
    "**Frequency**: Very High\n",
    "\n",
    "**Scenario**: Build an ETL pipeline that processes raw data, applies transformations, and loads results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Raw data simulation\n",
    "val rawLogs = Seq(\n",
    "  \"2023-01-15 10:30:00,user1,login,success\",\n",
    "  \"2023-01-15 10:35:00,user2,login,failed\",\n",
    "  \"2023-01-15 10:40:00,user1,purchase,299.99\",\n",
    "  \"2023-01-15 10:45:00,user3,login,success\",\n",
    "  \"2023-01-15 10:50:00,user2,purchase,149.99\",\n",
    "  \"2023-01-15 10:55:00,user1,logout,success\"\n",
    ")\n",
    "\n",
    "// FIXME: Implement ETL pipeline\n",
    "// Extract: Parse raw logs\n",
    "// Transform: Clean, validate, enrich data\n",
    "// Load: Aggregate and save results\n",
    "\n",
    "println(\"ETL Pipeline Implementation:\")\n",
    "\n",
    "// Extract phase\n",
    "val rawRDD = sc.parallelize(rawLogs)\n",
    "println(\"1. Extract - Raw data loaded\")\n",
    "\n",
    "// Transform phase\n",
    "val parsedData = ??? // Parse CSV-like data into structured format\n",
    "println(\"2. Transform - Data parsed and cleaned\")\n",
    "\n",
    "val enrichedData = ??? // Add derived columns (hour, day, etc.)\n",
    "println(\"3. Transform - Data enriched with derived fields\")\n",
    "\n",
    "val validatedData = ??? // Filter invalid records\n",
    "println(\"4. Transform - Data validated\")\n",
    "\n",
    "// Load phase\n",
    "val summaryStats = ??? // Calculate summary statistics\n",
    "println(\"5. Load - Summary statistics calculated\")\n",
    "\n",
    "// Display results\n",
    "println(\"\\nFinal Results:\")\n",
    "summaryStats.collect().foreach(println)\n",
    "\n",
    "println(\"\\nüí° ETL Best Practices Demonstrated:\")\n",
    "println(\"   - Modular design (Extract/Transform/Load)\")\n",
    "println(\"   - Data validation and cleaning\")\n",
    "println(\"   - Efficient transformations\")\n",
    "println(\"   - Summary aggregations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõë Cleanup\n",
    "\n",
    "**Clean up resources.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Stop SparkSession\n",
    "spark.stop()\n",
    "println(\"üõë Interview Practice Session Stopped\")\n",
    "println(\"‚úÖ All resources cleaned up!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Interview Preparation Tips\n",
    "\n",
    "**üéØ Key Points for Spark Interviews:**\n",
    "\n",
    "### **Technical Concepts:**\n",
    "- **RDD vs DataFrame vs Dataset**: Know when to use each\n",
    "- **Transformations vs Actions**: Lazy evaluation understanding\n",
    "- **Shuffle Operations**: groupByKey vs reduceByKey performance\n",
    "- **Caching Strategies**: When and how to cache data\n",
    "- **Partitioning**: repartition vs coalesce\n",
    "\n",
    "### **Common Questions:**\n",
    "1. **Word Count**: Every interview starts here\n",
    "2. **Performance Issues**: How to optimize slow jobs\n",
    "3. **Data Skew**: Causes and solutions\n",
    "4. **Window Functions**: Ranking and analytics\n",
    "5. **ETL Design**: Pipeline architecture\n",
    "\n",
    "### **Behavioral Questions:**\n",
    "- **Problem-Solving**: How do you debug Spark issues?\n",
    "- **Architecture**: How do you design scalable pipelines?\n",
    "- **Performance**: How do you handle large datasets?\n",
    "- **Best Practices**: Code organization and testing\n",
    "\n",
    "### **Preparation Strategy:**\n",
    "1. **Master Fundamentals**: RDD operations, DataFrame API\n",
    "2. **Practice Coding**: Implement common algorithms\n",
    "3. **Study Performance**: Understand optimization techniques\n",
    "4. **Learn Patterns**: Common ETL and analytics patterns\n",
    "5. **Mock Interviews**: Practice explaining your solutions\n",
    "\n",
    "**Remember**: Spark interviews test both coding skills and system understanding. Be ready to explain WHY you chose certain approaches!\n",
    "\n",
    "**üöÄ Good luck with your Spark interviews!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
