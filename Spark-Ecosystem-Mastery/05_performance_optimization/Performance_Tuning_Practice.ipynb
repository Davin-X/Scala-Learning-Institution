{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âš¡ Performance Optimization & Tuning\n",
    "\n",
    "**Phase 5: Performance Mastery - Optimizing Spark for Speed & Scale**\n",
    "\n",
    "**Prerequisites**: [04_spark_sql/Spark_SQL_Practice.ipynb](../04_spark_sql/Spark_SQL_Practice.ipynb)\n",
    "\n",
    "**Estimated time**: 60-75 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Goals\n",
    "\n",
    "By the end of this notebook, you'll be able to:\n",
    "- âœ… Understand and minimize shuffle operations\n",
    "- âœ… Optimize data partitioning strategies\n",
    "- âœ… Use caching and persistence effectively\n",
    "- âœ… Tune Spark configuration for performance\n",
    "- âœ… Identify and resolve data skew issues\n",
    "- âœ… Monitor and troubleshoot slow jobs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ Setup & Performance Monitoring\n",
    "\n",
    "**Initialize Spark and set up performance monitoring.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Import Spark libraries\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create SparkSession with performance monitoring\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"Performance Tuning Practice\")\n",
    "  .master(\"local[*]\")\n",
    "  .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "  .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "  .getOrCreate()\n",
    "\n",
    "val sc = spark.sparkContext\n",
    "\n",
    "println(\"ðŸš€ Performance Tuning Practice Started\")\n",
    "println(s\"Available cores: ${sc.defaultParallelism}\")\n",
    "println(s\"Default parallelism: ${sc.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Performance monitoring function\n",
    "def timeOperation[T](operationName: String)(block: => T): T = {\n",
    "  val startTime = System.nanoTime()\n",
    "  val result = block\n",
    "  val endTime = System.nanoTime()\n",
    "  val durationMs = (endTime - startTime) / 1e6\n",
    "  println(f\"âš¡ $operationName completed in $durationMs%.2f ms\")\n",
    "  result\n",
    "}\n",
    "\n",
    "// Create large test dataset\n",
    "val largeDataset = sc.parallelize(1 to 100000).map(i => (s\"Key${i % 100}\", i))\n",
    "println(s\"Created dataset with ${largeDataset.count()} records\")\n",
    "println(s\"Partitions: ${largeDataset.getNumPartitions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”€ Understanding Shuffle Operations\n",
    "\n",
    "**Shuffles are expensive - learn to minimize them!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Demonstrate shuffle operations\n",
    "println(\"ðŸ”€ Shuffle Operations Analysis:\")\n",
    "\n",
    "// Operations that cause shuffles\n",
    "val data = sc.parallelize(Seq(\n",
    "  (\"A\", 1), (\"B\", 2), (\"A\", 3), (\"B\", 4), (\"C\", 5)\n",
    "))\n",
    "\n",
    "println(\"Original data:\")\n",
    "data.collect().foreach(println)\n",
    "\n",
    "// groupByKey - CAUSES SHUFFLE\n",
    "val grouped = timeOperation(\"groupByKey\") {\n",
    "  data.groupByKey().collect()\n",
    "}\n",
    "println(\"groupByKey result:\")\n",
    "grouped.foreach { case (k, v) => println(s\"$k -> ${v.mkString(\",\")}\") }\n",
    "\n",
    "// reduceByKey - MORE EFFICIENT (combines locally first)\n",
    "val reduced = timeOperation(\"reduceByKey\") {\n",
    "  data.reduceByKey(_ + _).collect()\n",
    "}\n",
    "println(\"\\nreduceByKey result:\")\n",
    "reduced.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Caching & Persistence\n",
    "\n",
    "**Cache frequently used data to avoid recomputation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Demonstrate caching benefits\n",
    "println(\"ðŸ“¦ Caching & Persistence:\")\n",
    "\n",
    "val expensiveComputation = sc.parallelize(1 to 10000)\n",
    "  .map(x => (x % 100, x))\n",
    "  .reduceByKey(_ + _)\n",
    "  .filter(_._2 > 50000)\n",
    "\n",
    "// First access - no caching\n",
    "val firstAccess = timeOperation(\"First access (no cache)\") {\n",
    "  expensiveComputation.collect()\n",
    "}\n",
    "println(s\"First access result count: ${firstAccess.length}\")\n",
    "\n",
    "// Cache the RDD\n",
    "expensiveComputation.cache()\n",
    "println(\"\\nðŸ“¦ RDD cached in memory\")\n",
    "\n",
    "// Second access - from cache\n",
    "val secondAccess = timeOperation(\"Second access (from cache)\") {\n",
    "  expensiveComputation.collect()\n",
    "}\n",
    "println(s\"Second access result count: ${secondAccess.length}\")\n",
    "\n",
    "println(\"\\nðŸ’¡ Caching benefits:\")\n",
    "println(\"   - Avoids recomputation of expensive operations\")\n",
    "println(\"   - Speeds up iterative algorithms\")\n",
    "println(\"   - Useful for frequently accessed data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ† Performance Optimization Exercises\n",
    "\n",
    "**Apply performance tuning techniques to optimize Spark jobs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Exercise 1: Optimize Word Count\n",
    "// FIXME: Optimize this word count implementation\n",
    "\n",
    "println(\"ðŸ“ Performance Optimization Exercise:\")\n",
    "\n",
    "val text = sc.parallelize(Seq(\n",
    "  \"hello world hello spark\",\n",
    "  \"world spark is great\"\n",
    ") * 50)\n",
    "\n",
    "// Original implementation\n",
    "val original = timeOperation(\"Original word count\") {\n",
    "  text.flatMap(_.split(\" \"))\n",
    "      .map(word => (word, 1))\n",
    "      .reduceByKey(_ + _)\n",
    "      .collect()\n",
    "}\n",
    "\n",
    "// FIXME: Create optimized version\n",
    "val optimized = timeOperation(\"Optimized word count\") {\n",
    "  // Your optimized implementation here\n",
    "  original // Placeholder\n",
    "}\n",
    "\n",
    "println(\"\\nðŸ’¡ Optimizations to consider:\")\n",
    "println(\"   - Use coalesce() to reduce partitions\")\n",
    "println(\"   - Filter empty strings\")\n",
    "println(\"   - Case normalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ›‘ Cleanup\n",
    "\n",
    "**Clean up cached data and stop SparkSession.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Stop SparkSession\n",
    "spark.stop()\n",
    "println(\"ðŸ›‘ Performance Tuning Session Stopped\")\n",
    "println(\"âœ… All resources cleaned up!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š What Next?\n",
    "\n",
    "**ðŸŽ‰ Congratulations!** You've mastered Spark performance optimization!\n",
    "\n",
    "**Key Concepts:**\n",
    "- âœ… Shuffle operations and their costs\n",
    "- âœ… Caching and persistence strategies\n",
    "- âœ… Data partitioning and skew handling\n",
    "\n",
    "**Next:** Move to **06_streaming/** for real-time processing!\n",
    "\n",
    "**Remember:** Performance optimization is critical for production Spark jobs! âš¡"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
