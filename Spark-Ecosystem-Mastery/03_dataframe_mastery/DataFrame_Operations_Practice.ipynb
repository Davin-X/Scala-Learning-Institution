{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä DataFrame Operations & Transformations\n",
    "\n",
    "**Phase 3: DataFrame Mastery - Structured Data Processing**\n",
    "\n",
    "**Prerequisites**: [02_rdd_mastery/RDD_Transformations_Practice.ipynb](../02_rdd_mastery/RDD_Transformations_Practice.ipynb)\n",
    "\n",
    "**Estimated time**: 60-75 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Goals\n",
    "\n",
    "By the end of this notebook, you'll be able to:\n",
    "- ‚úÖ Create DataFrames from various data sources (CSV, JSON, RDDs)\n",
    "- ‚úÖ Perform column operations and expressions\n",
    "- ‚úÖ Apply filtering, sorting, and aggregation operations\n",
    "- ‚úÖ Handle missing data and data types\n",
    "- ‚úÖ Use window functions for advanced analytics\n",
    "- ‚úÖ Optimize DataFrame operations for performance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Setup & Data Sources\n",
    "\n",
    "**Initialize Spark and explore different DataFrame creation methods.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Import Spark libraries\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create SparkSession\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"DataFrame Operations Practice\")\n",
    "  .master(\"local[*]\")\n",
    "  .getOrCreate()\n",
    "\n",
    "println(\"üöÄ DataFrame Operations Practice Session Started\")\n",
    "println(s\"Spark Version: ${spark.version}\")\n",
    "println(s\"Scala Version: ${scala.util.Properties.versionString}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create sample datasets\n",
    "val employees = Seq(\n",
    "  (1, \"Alice\", \"Engineering\", 75000, \"2020-01-15\"),\n",
    "  (2, \"Bob\", \"Engineering\", 80000, \"2019-03-22\"),\n",
    "  (3, \"Charlie\", \"Sales\", 65000, \"2021-07-10\"),\n",
    "  (4, \"Diana\", \"Engineering\", 90000, \"2018-11-05\"),\n",
    "  (5, \"Eve\", \"Marketing\", 70000, \"2020-09-18\"),\n",
    "  (6, \"Frank\", \"Sales\", 72000, \"2019-12-08\"),\n",
    "  (7, \"Grace\", \"Engineering\", 85000, \"2021-02-28\"),\n",
    "  (8, \"Henry\", \"Marketing\", 68000, \"2020-06-14\")\n",
    ")\n",
    "\n",
    "println(\"üìä Sample datasets created for practice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è DataFrame Creation\n",
    "\n",
    "**Multiple ways to create DataFrames from different data sources.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Method 1: From Seq with toDF()\n",
    "println(\"üèóÔ∏è DataFrame Creation Methods:\")\n",
    "\n",
    "val employeesDF = employees.toDF(\"id\", \"name\", \"department\", \"salary\", \"hire_date\")\n",
    "println(\"\\n1. Created from Seq with toDF():\")\n",
    "employeesDF.show(5)\n",
    "employeesDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Method 2: From RDD\n",
    "val employeesRDD = spark.sparkContext.parallelize(employees)\n",
    "val employeesFromRDD = spark.createDataFrame(employeesRDD)\n",
    "  .toDF(\"id\", \"name\", \"department\", \"salary\", \"hire_date\")\n",
    "\n",
    "println(\"\\n2. Created from RDD:\")\n",
    "employeesFromRDD.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Column Operations & Expressions\n",
    "\n",
    "**Working with DataFrame columns using expressions and functions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Column selection and basic operations\n",
    "println(\"üìä Column Operations:\")\n",
    "\n",
    "val basicOps = employeesDF.select(\n",
    "  $\"name\",\n",
    "  $\"department\",\n",
    "  $\"salary\",\n",
    "  ($\"salary\" * 1.1).as(\"salary_with_bonus\"),\n",
    "  year(to_date($\"hire_date\")).as(\"hire_year\")\n",
    ")\n",
    "\n",
    "println(\"Basic column operations:\")\n",
    "basicOps.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// String operations\n",
    "println(\"\\nüî§ String Operations:\")\n",
    "val stringOps = employeesDF.select(\n",
    "  $\"name\",\n",
    "  length($\"name\").as(\"name_length\"),\n",
    "  upper($\"name\").as(\"name_upper\"),\n",
    "  lower($\"department\").as(\"dept_lower\"),\n",
    "  concat($\"name\", lit(\" - \"), $\"department\").as(\"name_dept\")\n",
    ")\n",
    "\n",
    "stringOps.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Filtering, Sorting & Selection\n",
    "\n",
    "**Powerful operations for data filtering and ordering.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Filtering operations\n",
    "println(\"üîç Filtering Operations:\")\n",
    "\n",
    "// Simple filters\n",
    "val engineers = employeesDF.filter($\"department\" === \"Engineering\")\n",
    "println(\"Engineering employees:\")\n",
    "engineers.show()\n",
    "\n",
    "val highEarners = employeesDF.filter($\"salary\" > 75000)\n",
    "println(\"\\nHigh earners (>75k):\")\n",
    "highEarners.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Sorting operations\n",
    "println(\"\\nüîÑ Sorting Operations:\")\n",
    "\n",
    "// Sort by single column\n",
    "val sortedBySalary = employeesDF.orderBy($\"salary\".desc)\n",
    "println(\"Sorted by salary (descending):\")\n",
    "sortedBySalary.select(\"name\", \"salary\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Aggregation Operations\n",
    "\n",
    "**Group and aggregate data for analytics and reporting.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Grouped aggregations\n",
    "println(\"üìä Grouped Aggregations:\")\n",
    "\n",
    "val deptStats = employeesDF.groupBy(\"department\")\n",
    "  .agg(\n",
    "    count(\"id\").as(\"employee_count\"),\n",
    "    sum(\"salary\").as(\"total_salary\"),\n",
    "    avg(\"salary\").as(\"avg_salary\"),\n",
    "    max(\"salary\").as(\"max_salary\"),\n",
    "    min(\"salary\").as(\"min_salary\")\n",
    "  )\n",
    "  .orderBy(\"department\")\n",
    "\n",
    "println(\"Department statistics:\")\n",
    "deptStats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÜ Practice Exercises\n",
    "\n",
    "**Apply DataFrame operations to solve real-world problems.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Exercise 1: Employee Analytics\n",
    "// FIXME: Implement employee analytics\n",
    "// 1. Department with highest average salary\n",
    "// 2. Employees hired in 2021 or later\n",
    "// 3. Salary distribution by location\n",
    "// 4. Top 3 highest paid employees\n",
    "// 5. Department with most employees\n",
    "\n",
    "println(\"üíº Employee Analytics Exercise:\")\n",
    "\n",
    "// 1. Department with highest average salary\n",
    "val deptAvgSalary = ??? // Hint: groupBy + agg + orderBy\n",
    "println(\"\\n1. Department with highest average salary:\")\n",
    "deptAvgSalary.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõë Cleanup\n",
    "\n",
    "**Always stop your SparkSession to free resources!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Stop SparkSession\n",
    "spark.stop()\n",
    "println(\"üõë Spark Session Stopped\")\n",
    "println(\"‚úÖ All resources cleaned up!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö What Next?\n",
    "\n",
    "**üéâ Congratulations!** You've mastered DataFrame operations!\n",
    "\n",
    "**You've learned:**\n",
    "- ‚úÖ Creating DataFrames from various sources\n",
    "- ‚úÖ Column operations and expressions\n",
    "- ‚úÖ Filtering, sorting, and aggregation\n",
    "- ‚úÖ Join operations between DataFrames\n",
    "\n",
    "**Next Steps:**\n",
    "1. Complete all exercises with your own implementations\n",
    "2. Move to **04_spark_sql/** for SQL-based processing\n",
    "3. Explore **05_performance_optimization/** for tuning\n",
    "\n",
    "**Remember:** DataFrames make Spark accessible to SQL users! ‚ö°"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
