{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåä Reactive Streams & Async Patterns\n",
    "\n",
    "**Phase 3: System Design - Event-Driven Architecture**\n",
    "\n",
    "**Master event sourcing, CQRS, reactive streams, and asynchronous communication patterns**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Reactive Streams Overview\n",
    "println(\"üåä REACTIVE STREAMS & ASYNC PATTERNS - EVENT-DRIVEN SYSTEMS\")\n",
    "println()\n",
    "\n",
    "println(\"üéØ Reactive Manifesto Principles:\")\n",
    "println(\"‚úì Responsive: Systems respond in timely fashion\")\n",
    "println(\"‚úì Resilient: Systems remain functional despite failures\")\n",
    "println(\"‚úì Elastic: Systems handle varying workloads\")\n",
    "println(\"‚úì Message-Driven: Async message passing for decoupling\")\n",
    "println()\n",
    "\n",
    "println(\"üîß Core Reactive Patterns:\")\n",
    "println(\"‚úì Event Sourcing: State as series of events\")\n",
    "println(\"‚úì CQRS: Separate read/write models\")\n",
    "println(\"‚úì Reactive Streams: Non-blocking backpressure\")\n",
    "println(\"‚úì Event Streaming: Continuous event processing\")\n",
    "println(\"‚úì Lambda Architecture: Batch + speed layer integration\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì° Event Sourcing Architecture\n",
    "\n",
    "**Complete state reconstruction from domain events - immutable audit logging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Event Sourcing Implementation\n",
    "import cats.effect.{IO, Ref}\n",
    "import cats.implicits._\n",
    "\n",
    "// Domain Events\n",
    "sealed trait DomainEvent {\n",
    "  def aggregateId: String\n",
    "  def sequenceNumber: Long\n",
    "  def timestamp: java.time.Instant\n",
    "}\n",
    "\n",
    "case class AccountCreated(\n",
    "  aggregateId: String,\n",
    "  ownerId: String,\n",
    "  currency: String,\n",
    "  sequenceNumber: Long = 1L,\n",
    "  timestamp: java.time.Instant = java.time.Instant.now()\n",
    ") extends DomainEvent\n",
    "\n",
    "case class FundsDeposited(\n",
    "  aggregateId: String,\n",
    "  amount: BigDecimal,\n",
    "  reference: String,\n",
    "  sequenceNumber: Long,\n",
    "  timestamp: java.time.Instant = java.time.Instant.now()\n",
    ") extends DomainEvent\n",
    "\n",
    "case class FundsWithdrawn(\n",
    "  aggregateId: String,\n",
    "  amount: BigDecimal,\n",
    "  reference: String,\n",
    "  sequenceNumber: Long,\n",
    "  timestamp: java.time.Instant = java.time.Instant.now()\n",
    ") extends DomainEvent\n",
    "\n",
    "// Current State (Projection)\n",
    "case class BankAccount(\n",
    "  id: String,\n",
    "  ownerId: String,\n",
    "  balance: BigDecimal,\n",
    "  currency: String,\n",
    "  version: Long = 0L\n",
    ")\n",
    "\n",
    "// Event Store\n",
    "trait EventStore[F[_]] {\n",
    "  def saveEvents(events: List[DomainEvent]): F[Unit]\n",
    "  def getEvents(aggregateId: String): F[List[DomainEvent]]\n",
    "  def getEventsSince(aggregateId: String, fromSequence: Long): F[List[DomainEvent]]\n",
    "}\n",
    "\n",
    "// In-memory event store implementation\n",
    "class InMemoryEventStore[F[_]: Async] extends EventStore[F] {\n",
    "  private val events = Ref.unsafe[F, Map[String, List[DomainEvent]]](Map.empty)\n",
    "  \n",
    "  def saveEvents(events: List[DomainEvent]): F[Unit] = {\n",
    "    events.groupBy(_.aggregateId).toList.traverse { case (aggId, newEvents) =>\n",
    "      events.modify { current =>\n",
    "        val existing = current.getOrElse(aggId, Nil)\n",
    "        val updated = existing ++ newEvents\n",
    "        (current + (aggId -> updated), ())\n",
    "      }\n",
    "    }.void\n",
    "  }\n",
    "  \n",
    "  def getEvents(aggregateId: String): F[List[DomainEvent]] =\n",
    "    events.get.map(_.getOrElse(aggregateId, Nil))\n",
    "    \n",
    "  def getEventsSince(aggregateId: String, fromSequence: Long): F[List[DomainEvent]] =\n",
    "    getEvents(aggregateId).map(_.filter(_.sequenceNumber > fromSequence))\n",
    "}\n",
    "\n",
    "println(\"üì° Event Sourcing Implemented\")\n",
    "println(\"‚Ä¢ Immutable event storage with replay capability\")\n",
    "println(\"‚Ä¢ Domain events representing state changes\")\n",
    "println(\"‚Ä¢ Sequence numbering for ordering guarantees\")\n",
    "println(\"‚Ä¢ Audit trail and debugging capabilities\")\n",
    "println()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ CQRS Architecture Pattern\n",
    "\n",
    "**Separate Command and Query responsibility for optimized read/write patterns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// CQRS Implementation - Separate Read/Write Models\n",
    "\n",
    "// Commands (Write Operations)\n",
    "sealed trait AccountCommand {\n",
    "  def accountId: String\n",
    "}\n",
    "\n",
    "case class OpenAccount(accountId: String, ownerId: String, currency: String) extends AccountCommand\n",
    "case class DepositFunds(accountId: String, amount: BigDecimal) extends AccountCommand\n",
    "case class WithdrawFunds(accountId: String, amount: BigDecimal) extends AccountCommand\n",
    "case class CloseAccount(accountId: String) extends AccountCommand\n",
    "\n",
    "// Queries (Read Operations)\n",
    "sealed trait AccountQuery {\n",
    "  def accountId: String\n",
    "}\n",
    "\n",
    "case class GetAccountBalance(accountId: String) extends AccountQuery\n",
    "case class GetAccountHistory(accountId: String, since: java.time.Instant) extends AccountQuery\n",
    "case class GetAccountSummary(accountId: String) extends AccountQuery\n",
    "\n",
    "// Read Models (Projections)\n",
    "case class AccountSummary(\n",
    "  accountId: String,\n",
    "  balance: BigDecimal,\n",
    "  currency: String,\n",
    "  ownerName: String,\n",
    "  lastActivity: java.time.Instant\n",
    ")\n",
    "\n",
    "case class AccountActivity(\n",
    "  accountId: String,\n",
    "  eventType: String,\n",
    "  amount: Option[BigDecimal],\n",
    "  timestamp: java.time.Instant,\n",
    "  reference: Option[String]\n",
    ")\n",
    "\n",
    "// CQRS System\n",
    "trait CommandHandler[F[_]] {\n",
    "  def handle(command: AccountCommand): F[Either[DomainError, List[DomainEvent]]]\n",
    "}\n",
    "\n",
    "trait QueryHandler[F[_]] {\n",
    "  def handle(query: AccountQuery): F[Either[DomainError, Any]]\n",
    "}\n",
    "\n",
    "class CQRSBankAccountSystem[F[_]: Async](\n",
    "  commandHandler: CommandHandler[F],\n",
    "  queryHandler: QueryHandler[F],\n",
    "  eventBus: EventBus[F]\n",
    ") {\n",
    "\n",
    "  def processCommand(command: AccountCommand): F[Either[DomainError, Unit]] =\n",
    "    for {\n",
    "      result <- commandHandler.handle(command)\n",
    "      _ <- result.traverse(events => eventBus.publish(events))\n",
    "    } yield result.map(_ => ())\n",
    "\n",
    "  def executeQuery[A](query: AccountQuery): F[Either[DomainError, A]] =\n",
    "    queryHandler.handle(query).map(_.map(_.asInstanceOf[A]))\n",
    "\n",
    "  // Optimized read methods\n",
    "  def getAccountBalance(accountId: String): F[Either[DomainError, BigDecimal]] =\n",
    "    executeQuery[BigDecimal](GetAccountBalance(accountId))\n",
    "\n",
    "  def getAccountSummary(accountId: String): F[Either[DomainError, AccountSummary]] =\n",
    "    executeQuery[AccountSummary](GetAccountSummary(accountId))\n",
    "\n",
    "  // Eventual consistency demonstration\n",
    "  def getAccountHistory(accountId: String): F[Either[DomainError, List[AccountActivity]]] =\n",
    "    executeQuery[List[AccountActivity]](\n",
    "      GetAccountHistory(accountId, java.time.Instant.now().minusSeconds(300))\n",
    "    )\n",
    "}\n",
    "\n",
    "sealed trait DomainError\n",
    "case class InsufficientFunds(accountId: String, requested: BigDecimal, available: BigDecimal) extends DomainError\n",
    "case class AccountNotFound(accountId: String) extends DomainError\n",
    "case class InvalidAmount(amount: BigDecimal) extends DomainError\n",
    "\n",
    "println(\"üîÑ CQRS Pattern Implemented\")\n",
    "println(\"‚Ä¢ Separate read/write models for optimization\")\n",
    "println(\"‚Ä¢ Commands for state-changing operations\")\n",
    "println(\"‚Ä¢ Queries for efficient data retrieval\")\n",
    "println(\"‚Ä¢ Event-driven communication between sides\")\n",
    "println()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåä Reactive Streams with Backpressure\n",
    "\n",
    "**Non-blocking, asynchronous stream processing with demand signaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Reactive Streams with Scala's fs2\n",
    "import fs2.{Stream, Pipe, Pull}\n",
    "import cats.effect.IO\n",
    "import scala.concurrent.duration._\n",
    "\n",
    "// Reactive stream processing pipeline\n",
    "case class DataBatch(\n",
    "  id: String,\n",
    "  data: List[String],\n",
    "  timestamp: java.time.Instant = java.time.Instant.now()\n",
    ")\n",
    "\n",
    "class ReactiveDataProcessor[F[_]: Async: Timer] {\n",
    "\n",
    "  // Producer: Simulates data source with varying rates\n",
    "  def dataSource: Stream[F, DataBatch] = {\n",
    "    Stream.unfoldEval(0) { counter =>\n",
    "      val batchSize = scala.util.Random.nextInt(100) + 50 // 50-150 items\n",
    "      val data = (1 to batchSize).map(i => s\"item_${counter}_$i\").toList\n",
    "      val batch = DataBatch(s\"batch_$counter\", data)\n",
    "      \n",
    "      for {\n",
    "        _ <- Timer[F].sleep((scala.util.Random.nextInt(500) + 100).millis) // Random delay\n",
    "      } yield Some((batch, counter + 1))\n",
    "    }.take(20) // Finite stream\n",
    "  }\n",
    "\n",
    "  // Processing pipeline with backpressure\n",
    "  def processingPipeline(input: Stream[F, DataBatch]): Stream[F, String] = {\n",
    "    input\n",
    "      . evalTap(batch => Async[F].delay(println(s\"üì¶ Received batch: ${batch.id}\")))\n",
    "      . map(processBatch)                          // Transform\n",
    "      . through(throttlingPipe)                   // Rate limiting\n",
    "      . through(bufferWithBackpressure)          // Buffering\n",
    "      . through(parallelProcessing(4))           // Parallel processing\n",
    "      . through(errorRecovery)                    // Resilience\n",
    "      . evalTap(result => Async[F].delay(println(s\"‚úÖ Processed: $result\")))\n",
    "  }\n",
    "\n",
    "  private def processBatch(batch: DataBatch): List[String] = {\n",
    "    batch.data.map(item => s\"processed_$item\")\n",
    "  }\n",
    "\n",
    "  // Throttling with backpressure\n",
    "  private def throttlingPipe: Pipe[F, List[String], List[String]] = {\n",
    "    _.groupWithin(1000, 2.seconds)  // Max 1000 items or 2 seconds\n",
    "      .map(_.flatten.take(1000))    // Limit batch size\n",
    "  }\n",
    "\n",
    "  // Buffer with backpressure signaling\n",
    "  private def bufferWithBackpressure: Pipe[F, List[String], List[String]] = {\n",
    "    _.bufferSliding(3)  // Buffer up to 3 elements, drop oldest when full\n",
    "  }\n",
    "\n",
    "  // Parallel processing with bounded parallelism\n",
    "  private def parallelProcessing(parallelism: Int): Pipe[F, List[String], String] = {\n",
    "    _.mapAsync(parallelism) { batch =>\n",
    "      Async[F].delay {\n",
    "        Thread.sleep(100) // Simulate processing time\n",
    "        s\"BATCH_SIZE_${batch.size}\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // Error recovery with fallback\n",
    "  private def errorRecovery: Pipe[F, String, String] = {\n",
    "    _.handleErrorWith { error =>\n",
    "      Stream.eval(Async[F].delay(println(s\"‚ùå Processing error: $error\"))) *>\n",
    "      Stream.emit(\"ERROR_RECOVERED\")\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // Complete reactive pipeline\n",
    "  def runPipeline(): Stream[F, Unit] = {\n",
    "    processingPipeline(dataSource)\n",
    "      .drain // Convert to Unit stream\n",
    "      .onComplete(Stream.eval(Async[F].delay(println(\"üéØ Pipeline completed successfully\"))))\n",
    "  }\n",
    "}\n",
    "\n",
    "println(\"üåä Reactive Streams with Backpressure Implemented\")\n",
    "println(\"‚Ä¢ Non-blocking stream processing\")\n",
    "println(\"‚Ä¢ Automatic backpressure handling\")\n",
    "println(\"‚Ä¢ Throttling and rate limiting\")\n",
    "println(\"‚Ä¢ Parallel processing with bounded concurrency\")\n",
    "println(\"‚Ä¢ Error recovery and resilience\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì° Event Streaming with Kafka Patterns\n",
    "\n",
    "**Distributed event streaming and real-time data processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Event Streaming Architecture\n",
    "println(\"üì° EVENT STREAMING & REAL-TIME PROCESSING\")\n",
    "println()\n",
    "\n",
    "// Stream processing patterns\n",
    "println(\"üî• Stream Processing Patterns:\")\n",
    "println()\n",
    "\n",
    "println(\"1. üìä Windowing Operations\")\n",
    "println(\"   ‚úì Tumbling windows: Fixed-size, non-overlapping\")\n",
    "println(\"   ‚úì Sliding windows: Overlapping with fixed advances\")\n",
    "println(\"   ‚úì Session windows: Based on activity gaps\")\n",
    "println(\"   ‚úì Time-based windows: Calendar-aligned intervals\")\n",
    "println()\n",
    "\n",
    "println(\"2. üîó Stream Joins\")\n",
    "println(\"   ‚úì Inner joins: Correlated events only\")\n",
    "println(\"   ‚úì Left joins: All left events with optional right\")\n",
    "println(\"   ‚úì Full outer joins: All events from both streams\")\n",
    "println(\"   ‚úì Interval joins: Events within time windows\")\n",
    "println()\n",
    "\n",
    "println(\"3. üîÑ Stateful Operations\")\n",
    "println(\"   ‚úì Stateful aggregations with fault tolerance\")\n",
    "println(\"   ‚úì Checkpointing for recoverable state\")\n",
    "println(\"   ‚úì State stores with TTL expiration\")\n",
    "println(\"   ‚úì Interactive queries on stream state\")\n",
    "println()\n",
    "\n",
    "println(\"4. üìà Event Enrichment\")\n",
    "println(\"   ‚úì Lookups in external data sources\")\n",
    "println(\"   ‚úì Joining with reference data\")\n",
    "println(\"   ‚úì Geolocation and entity resolution\")\n",
    "println(\"   ‚úì Real-time feature engineering\")\n",
    "println()\n",
    "\n",
    "println(\"üèóÔ∏è Lambda Architecture Integration:\")\n",
    "println(\"‚Ä¢ Batch layer: Historical data processing\")\n",
    "println(\"‚Ä¢ Speed layer: Real-time event processing\")\n",
    "println(\"‚Ä¢ Serving layer: Unified data access\")\n",
    "println(\"‚Ä¢ Kappa architecture: Stream-first approach\")\n",
    "println()\n",
    "\n",
    "println(\"‚ö° Reactive System Guarantees:\")\n",
    "println(\"‚Ä¢ At-least-once processing\")\n",
    "println(\"‚Ä¢ Exactly-once semantics\")\n",
    "println(\"‚Ä¢ Event time vs. processing time\")\n",
    "println(\"‚Ä¢ Out-of-order event handling\")\n",
    "println(\"‚Ä¢ Watermarking for completion\")\n",
    "\n",
    "println(\"\\nThe key to reactive streams: handle data flow, not just individual elements!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
