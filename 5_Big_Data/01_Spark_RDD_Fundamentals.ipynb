{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âš¡ Apache Spark RDD Fundamentals\n",
    "\n",
    "**Phase 5: Big Data Processing (RDD Module 1)**\n",
    "\n",
    "**Prerequisites**: Basic Scala collections and functional programming\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”— Spark RDD Architecture\n",
    "\n",
    "Understanding Resilient Distributed Datasets (RDD) - the fundamental abstraction of Apache Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Import Spark dependencies (for notebook environment)\n",
    "// In a real environment, this would run on Spark cluster\n",
    "import scala.collection.parallel.ParSeq\n",
    "\n",
    "// RDD characteristics simulation in Scala collections\n",
    "case class RDD[T](data: ParSeq[T], partitions: Int) {\n",
    "  \n",
    "  // RDD properties\n",
    "  lazy val partitionsData = data.grouped(data.size / partitions + 1).toList\n",
    "  \n",
    "  def map[U](f: T => U): RDD[U] = {\n",
    "    copy(data = data.map(f))\n",
    "  }\n",
    "  \n",
    "  def filter(predicate: T => Boolean): RDD[T] = {\n",
    "    copy(data = data.filter(predicate))\n",
    "  }\n",
    "  \n",
    "  def collect(): List[T] = data.toList\n",
    "  \n",
    "  // Simulate distributed operations\n",
    "  def reduce(f: (T, T) => T): Option[T] = {\n",
    "    if (data.isEmpty) None\n",
    "    else Some(data.reduce(f))\n",
    "  }\n",
    "  \n",
    "  def count(): Long = data.size.toLong\n",
    "  \n",
    "  def distinct(): RDD[T] = {\n",
    "    copy(data = data.distinct)\n",
    "  }\n",
    "}\n",
    "\n",
    "// Companion object for RDD creation\n",
    "object RDD {\n",
    "  def fromSeq[T](seq: Seq[T], partitions: Int = 4): RDD[T] = {\n",
    "    RDD(seq.par, partitions)\n",
    "  }\n",
    "\n",
    "  def range(start: Long, end: Long, partitions: Int = 4): RDD[Long] = {\n",
    "    val data = (start until end).par\n",
    "    RDD(data, partitions)\n",
    "  }\n",
    "}\n",
    "\n",
    "// Test RDD creation and basic operations\n",
    "val numbersRDD = RDD.fromSeq((1 to 20).toList)\n",
    "println(s\"RDD created with ${numbersRDD.count()} elements in ${numbersRDD.partitions} partitions\")\n",
    "println(s\"Partition data: ${numbersRDD.partitionsData.map(_.size)}\")\n",
    "println()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ RDD Transformations vs Actions\n",
    "\n",
    "Understanding the fundamental difference between transformations and actions in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// RDD Operation Categories\n",
    "case class RDDAnalysis[T](rdd: RDD[T]) {\n",
    "  \n",
    "  // Transformations (lazy) - return new RDD\n",
    "  def mapTransform[U](f: T => U): RDD[U] = {\n",
    "    println(s\"[TRANSFORMATION] mapTransform applying function\")\n",
    "    rdd.map(f)\n",
    "  }\n",
    "  \n",
    "  def filterTransform(predicate: T => Boolean): RDD[T] = {\n",
    "    println(s\"[TRANSFORMATION] filterTransform applying predicate\")\n",
    "    rdd.filter(predicate)\n",
    "  }\n",
    "  \n",
    "  def flatMapTransform[U](f: T => Iterable[U]): RDD[U] = {\n",
    "    println(s\"[TRANSFORMATION] flatMapTransform applying function\")\n",
    "    RDD(rdd.data.flatMap(f), rdd.partitions)\n",
    "  }\n",
    "  \n",
    "  // Actions (eager) - compute and return results\n",
    "  def countAction(): Long = {\n",
    "    println(s\"[ACTION] countAction collecting ${rdd.data.size} elements\")\n",
    "    rdd.count()\n",
    "  }\n",
    "  \n",
    "  def collectAction(): List[T] = {\n",
    "    println(s\"[ACTION] collectAction returning all ${rdd.data.size} elements\")\n",
    "    rdd.collect()\n",
    "  }\n",
    "  \n",
    "  def reduceAction(f: (T, T) => T): Option[T] = {\n",
    "    println(s\"[ACTION] reduceAction combining ${rdd.data.size} elements\")\n",
    "    rdd.reduce(f)\n",
    "  }\n",
    "}\n",
    "\n",
    "// Demonstrate lazy vs eager evaluation\n",
    "val originalRDD = RDD.fromSeq(List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10))\n",
    "val analyzer = RDDAnalysis(originalRDD)\n",
    "\n",
    "println(\"=== LAZY TRANSFORMATIONS (no computation yet) ===\")\n",
    "val transformedRDD = analyzer\n",
    "  .mapTransform(_ * 2)\n",
    "  .filterTransform(_ > 10)\n",
    "  .mapTransform(_.toString)\n",
    "\n",
    "println(\"\\n=== EAGER ACTION (triggers computation) ===\")\n",
    "val result = analyzer.filterTransform(_ > 5).collectAction()\n",
    "println(s\"Final result: $result\")\n",
    "println()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ Advanced RDD Operations\n",
    "\n",
    "Key operations that make Spark RDDs powerful for big data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Word count example - classic big data problem\n",
    "def wordCountRDD(texts: Seq[String]): RDD[(String, Int)] = {\n",
    "  val textRDD = RDD.fromSeq(texts)\n",
    "  \n",
    "  val wordCounts = textRDD\n",
    "    .flatMapTransform(text => text.toLowerCase.split(\"\\\\W+\"))\n",
    "    .filterTransform(_.nonEmpty)\n",
    "    .mapTransform(word => (word, 1))\n",
    "    .collectAction()\n",
    "    .groupBy(_._1)\n",
    "    .mapValues(_.map(_._2).sum)\n",
    "  \n",
    "  // Convert back to RDD-like structure\n",
    "  RDD.fromSeq(wordCounts.toList)\n",
    "}\n",
    "\n",
    "// Set operations with RDDs\n",
    "def rddIntersection[T](rdd1: RDD[T], rdd2: RDD[T]): RDD[T] = {\n",
    "  val set1 = rdd1.collectAction().toSet\n",
    "  RDD.fromSeq(rdd2.collectAction().filter(set1.contains))\n",
    "}\n",
    "\n",
    "def rddUnion[T](rdd1: RDD[T], rdd2: RDD[T]): RDD[T] = {\n",
    "  RDD.fromSeq(rdd1.collectAction() ++ rdd2.collectAction())\n",
    "}\n",
    "\n",
    "def rddDistinct[T](rdd: RDD[T]): RDD[T] = {\n",
    "  RDD.fromSeq(rdd.collectAction().distinct)\n",
    "}\n",
    "\n",
    "// Sample data and operations\n",
    "val sampleTexts = Seq(\n",
    "  \"Hello world hello Scala\",\n",
    "  \"Scala is amazing Scala programming\",\n",
    "  \"Spark and Scala make big data easy\",\n",
    "  \"Hello Spark world\"\n",
    ")\n",
    "\n",
    "val wordCounts = wordCountRDD(sampleTexts)\n",
    "println(\"Word Count Results:\")\n",
    "wordCounts.collectAction()\n",
    "  .sortBy(-_._2)  // Sort by frequency descending\n",
    "  .foreach { case (word, count) =>\n",
    "    println(f\"  $word%15s : $count\")\n",
    "  }\n",
    "\n",
    "// Set operations example\n",
    "val rddA = RDD.fromSeq(Seq(1, 2, 3, 4, 5))\n",
    "val rddB = RDD.fromSeq(Seq(3, 4, 5, 6, 7))\n",
    "val rddC = RDD.fromSeq(Seq(1, 2, 8, 9, 10))\n",
    "\n",
    "println(\"\\nSet Operations:\")\n",
    "println(s\"A âˆ© B (intersection): ${rddIntersection(rddA, rddB).collectAction().sorted}\")\n",
    "println(s\"A âˆª B (union):        ${rddDistinct(rddUnion(rddA, rddB)).collectAction().sorted}\")\n",
    "println()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§© RDD Persistence & Caching\n",
    "\n",
    "Understanding when and how to cache RDDs for performance optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// RDD Caching simulation (in real Spark, this manages memory/disk persistence)\n",
    "import scala.collection.mutable.Map\n",
    "\n",
    "case class CachedRDD[T](\n",
    "  rdd: RDD[T],\n",
    "  storageLevel: StorageLevel = StorageLevel.NONE\n",
    ") {\n",
    "  private var cachedResult: Option[List[T]] = None\n",
    "  private var accessCount = 0\n",
    "  private var cacheHits = 0\n",
    "  \n",
    "  def cache(level: StorageLevel = StorageLevel.MEMORY_AND_DISK): CachedRDD[T] = {\n",
    "    println(s\"Caching RDD with storage level: $level\")\n",
    "    var cost = \"low\"\n",
    "    if (level == StorageLevel.MEMORY_AND_DISK) cost = \"medium\"\n",
    "    else if (level == StorageLevel.DISK_ONLY) cost = \"high\"\n",
    "    println(s\"Cache operation cost: $cost\")\n",
    "    this.copy(storageLevel = level)\n",
    "  }\n",
    "  \n",
    "  def unpersist(): CachedRDD[T] = {\n",
    "    cachedResult = None\n",
    "    println(\"Unpersisting RDD\")\n",
    "    this.copy(storageLevel = StorageLevel.NONE)\n",
    "  }\n",
    "  \n",
    "  def collectOptimized(): List[T] = {\n",
    "    accessCount += 1\n",
    "    \n",
    "    if (storageLevel != StorageLevel.NONE && cachedResult.isDefined) {\n",
    "      cacheHits += 1\n",
    "      println(s\"Cache hit! Using cached data ($cacheHits/$accessCount access ratio)\")\n",
    "      cachedResult.get\n",
    "    } else {\n",
    "      println(s\"Cache miss! Computing and ${if (storageLevel != StorageLevel.NONE) \"caching\" else \"not caching\"} result\")\n",
    "      val result = rdd.collect()\n",
    "      if (storageLevel != StorageLevel.NONE) {\n",
    "        cachedResult = Some(result)\n",
    "      }\n",
    "      result\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  def getStorageLevel: StorageLevel = storageLevel\n",
    "}\n",
    "\n",
    "// Storage level enumeration\n",
    "sealed trait StorageLevel\n",
    "object StorageLevel {\n",
    "  case object NONE extends StorageLevel\n",
    "  case object DISK_ONLY extends StorageLevel\n",
    "  case object MEMORY_ONLY extends StorageLevel\n",
    "  case object MEMORY_AND_DISK extends StorageLevel\n",
    "  \n",
    "  def toString(level: StorageLevel): String = level match {\n",
    "    case NONE => \"NONE\"\n",
    "    case DISK_ONLY => \"DISK_ONLY\"\n",
    "    case MEMORY_ONLY => \"MEMORY_ONLY\"\n",
    "    case MEMORY_AND_DISK => \"MEMORY_AND_DISK\"\n",
    "  }\n",
    "}\n",
    "\n",
    "// Demonstrate caching benefits\n",
    "val expensiveComputationRDD = RDD.fromSeq((1 to 10000).map(n => {\n",
    "  // Simulate expensive calculation\n",
    "  Thread.sleep(1)  // 1ms delay per element (would be computational work)\n",
    "  n * n\n",
    "}))\n",
    "\n",
    "val cachedRDD = CachedRDD(expensiveComputationRDD).cache(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "println(\"Cached RDD Performance Test:\")\n",
    "\n",
    "// First access - cache miss\n",
    "val start1 = System.nanoTime()\n",
    "val result1 = cachedRDD.collectOptimized().take(5)\n",
    "val time1 = System.nanoTime() - start1\n",
    "println(f\"First access:  ${time1/1e6}%.1f ms\")\n",
    "\n",
    "// Second access - cache hit\n",
    "val start2 = System.nanoTime()\n",
    "val result2 = cachedRDD.collectOptimized().take(5)\n",
    "val time2 = System.nanoTime() - start2\n",
    "println(f\"Second access: ${time2/1e6}%.1f ms (${time1/time2}%.1fx faster)\")\n",
    "\n",
    "// Third access - cache hit again\n",
    "val start3 = System.nanoTime()\n",
    "val result3 = cachedRDD.collectOptimized().take(5)\n",
    "val time3 = System.nanoTime() - start3\n",
    "println(f\"Third access:  ${time3/1e6}%.1f ms\")\n",
    "\n",
    "cachedRDD.unpersist()\n",
    "println()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ RDD Lineage & Fault Tolerance\n",
    "\n",
    "Understanding how Spark maintains data lineage for automatic fault recovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// RDD Lineage tracking (simplified DAG representation)\n",
    "case class RDDOperation(\n",
    "  operationType: String,\n",
    "  description: String,\n",
    "  inputRDDs: List[String] = Nil,\n",
    "  parameters: Map[String, Any] = Map.empty\n",
    ")\n",
    "\n",
    "case class RDDLineage(\n",
    "  currentRDD: String,\n",
    "  operations: List[RDDOperation],\n",
    "  dependencies: List[String]\n",
    ") {\n",
    "  \n",
    "  def addOperation(op: RDDOperation): RDDLineage = {\n",
    "    this.copy(\n",
    "      operations = operations :+ op,\n",
    "      dependencies = op.inputRDDs ::: dependencies\n",
    "    )\n",
    "  }\n",
    "  \n",
    "  def printLineage(): Unit = {\n",
    "    println(s\"Lineage for RDD: $currentRDD\")\n",
    "    println(s\"Dependencies: ${dependencies.mkString(\", \")}\")\n",
    "    println(\"Operations:\")\n",
    "    operations.zipWithIndex.foreach { case (op, idx) =>\n",
    "      println(f\"  ${idx+1}. ${op.operationType}%12s : ${op.description}\")\n",
    "      if (op.parameters.nonEmpty) {\n",
    "        println(f\"      Params: ${op.parameters.mkString(\", \")}\")\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  def lineageDepth: Int = operations.size\n",
    "  def isNarrowDependency(index: Int): Boolean = {\n",
    "    // Simplified - narrow if operation affects single partition\n",
    "    operations(index).operationType match {\n",
    "      case \"map\" | \"filter\" | \"flatMap\" => true\n",
    "      case \"reduceByKey\" | \"groupByKey\" => false\n",
    "      case _ => true\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "// Demonstrate RDD lineage tracking\n",
    "var lineage = RDDLineage(\"numbersRDD\", Nil, Nil)\n",
    "\n",
    "lineage = lineage.addOperation(\n",
    "  RDDOperation(\"parallelize\", \"Create parallel collection\", Nil, Map(\"elements\" -> 1000))\n",
    ")\n",
    "\n",
    "lineage = lineage.addOperation(\n",
    "  RDDOperation(\"map\", \"Multiply by 2\", List(\"numbersRDD\"), Map(\"function\" -> \"_ * 2\"))\n",
    ")\n",
    "\n",
    "lineage = lineage.addOperation(\n",
    "  RDDOperation(\"filter\", \"Keep even numbers\", List(lineage.currentRDD), Map(\"predicate\" -> \"_ % 2 == 0\"))\n",
    ")\n",
    "\n",
    "lineage = lineage.addOperation(\n",
    "  RDDOperation(\"reduce\", \"Sum all elements\", List(lineage.currentRDD), Map(\"combiner\" -> \"_ + _\"))\n",
    ")\n",
    "\n",
    "println(\"RDD Lineage Analysis:\")\n",
    "lineage.printLineage()\n",
    "println(f\"\\nLineage depth: ${lineage.lineageDepth}\")\n",
    "println(s\"Dependency types: ${lineage.operations.indices.map(i =>\n",
    "  s\"Op${i+1}=${if (lineage.isNarrowDependency(i)) \"narrow\" else \"wide\"}\").mkString(\", \")}\")\n",
    "println()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ RDD Best Practices & Performance\n",
    "\n",
    "### **Key RDD Optimization Principles:**\n",
    "\n",
    "1. **Minimize object creation in transformations**\n",
    "2. **Avoid unnecessary shuffling with wide dependencies**\n",
    "3. **Cache intermediate results when needed**\n",
    "4. **Prefer lazy evaluation benefits**\n",
    "5. **Use appropriate storage levels**\n",
    "\n",
    "### **When to Use RDDs vs DataFrames:**\n",
    "\n",
    "| Use Case | RDD | DataFrame |\n",
    "|----------|-----|-----------|\n",
    "| **Control** | Full control over everything | Limited to DF operations |\n",
    "| **Type Safety** | Compile-time type checking | Runtime schema validation |\n",
    "| **Performance** | Can optimize at low level | Catalyst optimizer works |\n",
    "| **Learning Curve** | Steeper (functional programming) | Easier (SQL-like) |\n",
    "| **ML Pipelines** | Requires MLlib | Integrated ML capabilities |\n",
    "\n",
    "### **RDD vs DataFrame Code Comparison:**\n",
    "\n",
    "**RDD Way:**\n",
    "```scala\n",
    "val rddResult = sc.textFile(\"data.txt\")\n",
    "  .map(line => line.split(\",\"))\n",
    "  .filter(fields => fields(0).toInt > 100)\n",
    "  .map(fields => (fields(1), fields(2).toDouble))\n",
    "  .reduceByKey(_ + _)\n",
    "```\n",
    "\n",
    "**DataFrame Way:**\n",
    "```scala\n",
    "val dfResult = spark.read.csv(\"data.txt\")\n",
    "  .filter(col(\"field0\") > 100)\n",
    "  .groupBy(\"field1\")\n",
    "  .sum(\"field2\")\n",
    "```\n",
    "\n",
    "### **Performance Tips:**\n",
    "\n",
    "1. **Partition Optimization**\n",
    "   - Avoid too many partitions (200-1000 is good)\n",
    "   - Avoid too few partitions (underutilizes cluster)\n",
    "   - Consider data locality\n",
    "\n",
    "2. **Memory Management**\n",
    "   - Use `persist(StorageLevel.MEMORY_AND_DISK_SER)` for large objects\n",
    "   - Use `unpersist()` when data no longer needed\n",
    "   - Be careful with closures (they capture whole objects)\n",
    "\n",
    "3. **Shuffle Optimization**\n",
    "   - Minimize wide transformations\n",
    "   - Use `coalesce()` to reduce partitions after filtering\n",
    "   - `repartition()` strategically\n",
    "\n",
    "### **Debugging RDD Operations:**\n",
    "\n",
    "```scala\n",
    "// Inspect RDD partitions\n",
    "rdd.mapPartitionsWithIndex((index, iter) => {\n",
    "  println(s\"Partition $index has ${iter.size} elements\")\n",
    "  iter\n",
    "}).collect()\n",
    "\n",
    "// Trace RDD lineage\n",
    "rdd.toDebugString\n",
    "\n",
    "// Sample data for debugging\n",
    "rdd.take(10).foreach(println)\n",
    "```\n",
    "\n",
    "### **RDD Fault Tolerance Strategy:**\n",
    "\n",
    "When a partition is lost:\n",
    "1. Spark identifies lost partition\n",
    "2. Recreates partition from lineage\n",
    "3. Only lost partition is recomputed\n",
    "4. Other partitions continue working\n",
    "\n",
    "```scala\n",
    "// Simulate fault tolerance\n",
    "val reliableRDD = sc.parallelize(1 to 1000, 10)\n",
    "  .map(n => n * 2)\n",
    "  .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "  .filter(_ > 100)  // Later operations can fail\n",
    "  .cache()          // Intermediate result cached\n",
    "```\n",
    "\n",
    "### **RDD Limitations:**\n",
    "\n",
    "1. **No Automatic Optimization** - Unlike DataFrames, no Catalyst optimizer\n",
    "2. **Type Safety Risk** - Can have runtime exceptions from type mismatches\n",
    "3. **Manual Optimization** - Developer must manually optimize execution\n",
    "4. **Verbose Code** - Functional operations can be more verbose than SQL\n",
    "5. **Debugging Complexity** - Harder to debug complex transformations\n",
    "\n",
    "### **When RDDs Excel:**\n",
    "\n",
    "- **Custom Partitioner Logic** - Need specific data distribution\n",
    "- **Low-Level Control** - Precise memory management needed\n",
    "- **Binary Data Processing** - Working with serialized objects\n",
    "- **Custom Algorithm Implementation** - Parallel graph algorithms, machine learning\n",
    "- **Real-Time Processing** - Integration with streaming systems\n",
    "\n",
    "**Next: DataFrame API and Spark SQL for more structured processing**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
