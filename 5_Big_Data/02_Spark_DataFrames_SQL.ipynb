{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Apache Spark DataFrames & SQL\n",
    "\n",
    "**Phase 5: Big Data Processing - DataFrame Module 2**\n",
    "\n",
    "**Prerequisites**: Phase 5.1 RDD Fundamentals, SQL knowledge\n",
    "\n",
    "**Industry Level**: Production ETL pipelines, data lake processing, BI analytics**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è DataFrame Architecture & Catalyst Optimizer\n",
    "\n",
    "Understanding how Spark's DataFrame API leverages Catalyst optimizer for industry performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Industry-standard imports for production Spark applications\n",
    "import org.apache.spark.sql.{SparkSession, DataFrame, Dataset}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import scala.util.Try\n",
    "import com.typesafe.config.ConfigFactory\n",
    "\n",
    "// Production DataFrame configuration\n",
    "case class DataPipelineConfig(\n",
    "  spark: SparkConfig,\n",
    "  input: DataSourceConfig,\n",
    "  output: DataSinkConfig,\n",
    "  transformations: List[TransformConfig]\n",
    ")\n",
    "\n",
    "case class SparkConfig(\n",
    "  master: String = \"local[*]\",\n",
    "  appName: String = \"IndustryDataPipeline\",\n",
    "  executorMemory: String = \"2g\",\n",
    "  driverMemory: String = \"1g\",\n",
    "  shufflePartitions: Int = 200,\n",
    "  catalogImplementation: String = \"hive\"\n",
    ")\n",
    "\n",
    "// Production SparkSession factory\n",
    "object SparkSessionFactory {\n",
    "  def createProductionSession(config: SparkConfig): SparkSession = {\n",
    "    SparkSession.builder()\n",
    "      .master(config.master)\n",
    "      .appName(config.appName)\n",
    "      .config(\"spark.executor.memory\", config.executorMemory)\n",
    "      .config(\"spark.driver.memory\", config.driverMemory)\n",
    "      .config(\"spark.sql.shuffle.partitions\", config.shufflePartitions.toString)\n",
    "      .config(\"spark.sql.catalogImplementation\", config.catalogImplementation)\n",
    "      // Production optimizations\n",
    "      .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "      .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "      .config(\"spark.sql.adaptive.coalescePartitions.minPartitionSize\", \"1MB\")\n",
    "      .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "      // Delta Lake integration for production\n",
    "      .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "      .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "      .enableHiveSupport()\n",
    "      .getOrCreate()\n",
    "  }\n",
    "}\n",
    "\n",
    "// Demonstrate type-safe DataFrame operations\n",
    "case class Employee(\n",
    "  id: Int,\n",
    "  name: String,\n",
    "  department: String,\n",
    "  salary: Double,\n",
    "  hireDate: String,\n",
    "  skills: Seq[String]\n",
    ")\n",
    "\n",
    "case class EmployeeStats(\n",
    "  department: String,\n",
    "  employeeCount: Int,\n",
    "  averageSalary: Double,\n",
    "  totalSalary: Double,\n",
    "  skillDiversity: Int\n",
    ")\n",
    "\n",
    "object DataFrameProcessor {\n",
    "\n",
    "  // Schema evolution with Delta Lake\n",
    "  val employeeSchema = StructType(Seq(\n",
    "    StructField(\"id\", IntegerType, false),\n",
    "    StructField(\"name\", StringType, false),\n",
    "    StructField(\"department\", StringType, true),\n",
    "    StructField(\"salary\", DoubleType, true),\n",
    "    StructField(\"hireDate\", DateType, true),\n",
    "    StructField(\"skills\", ArrayType(StringType), true),\n",
    "    StructField(\"partition_date\", DateType, true) // Time-based partitioning\n",
    "  ))\n",
    "\n",
    "  def loadEmployeeData(spark: SparkSession, path: String): DataFrame = {\n",
    "    spark.read\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"inferSchema\", \"false\")\n",
    "      .schema(employeeSchema)\n",
    "      .csv(path)\n",
    "      .withColumn(\"hire_year\", year(col(\"hireDate\")))\n",
    "      .cache() // Cache for repeated operations\n",
    "  }\n",
    "\n",
    "  // Advanced transformations with business logic\n",
    "  def calculateDepartmentStats(df: DataFrame): DataFrame = {\n",
    "    df.withColumn(\"salary_bucket\",\n",
    "        when(col(\"salary\") < 50000, \"entry\")\n",
    "        .when(col(\"salary\") < 100000, \"mid\")\n",
    "        .otherwise(\"senior\")\n",
    "      )\n",
    "      .groupBy(\"department\")\n",
    "      .agg(\n",
    "        count(\"id\").as(\"employee_count\"),\n",
    "        round(avg(\"salary\"), 2).as(\"avg_salary\"),\n",
    "        sum(\"salary\").as(\"total_salary\"),\n",
    "        max(\"salary\").as(\"max_salary\"),\n",
    "        min(\"salary\").as(\"min_salary\"),\n",
    "        collect_set(\"salary_bucket\").as(\"salary_buckets\"),\n",
    "        size(collect_set(\"salary_bucket\")).as(\"salary_bucket_count\")\n",
    "      )\n",
    "      .orderBy(desc(\"total_salary\"))\n",
    "  }\n",
    "\n",
    "  // Time-window analytics\n",
    "  def calculateExperienceBands(df: DataFrame): DataFrame = {\n",
    "    df.withColumn(\"years_experience\",\n",
    "        floor(datediff(current_date(), col(\"hireDate\")) / 365.25)\n",
    "      )\n",
    "      .withColumn(\"experience_band\",\n",
    "        when(col(\"years_experience\") < 2, \"Junior\")\n",
    "        .when(col(\"years_experience\") < 5, \"Mid-level\")\n",
    "        .when(col(\"years_experience\") < 8, \"Senior\")\n",
    "        .otherwise(\"Expert\")\n",
    "      )\n",
    "  }\n",
    "\n",
    "  // JSON data processing (common in modern ETL)\n",
    "  def processJsonData(spark: SparkSession, jsonPath: String): DataFrame = {\n",
    "    spark.read\n",
    "      .option(\"multiline\", \"true\")\n",
    "      .option(\"dateFormat\", \"yyyy-MM-dd\")\n",
    "      .json(jsonPath)\n",
    "      .select(\n",
    "        \"employee_id\",\n",
    "        \"project_name\",\n",
    "        \"hours_worked\",\n",
    "        \"start_date\",\n",
    "        explode(\"technologies_used\").as(\"technology\"),\n",
    "        \"performance_rating\"\n",
    "      )\n",
    "      .filter(\"hours_worked > 0\")\n",
    "      .groupBy(\"technology\")\n",
    "      .agg(\n",
    "        countDistinct(\"employee_id\").as(\"unique_contributors\"),\n",
    "        sum(\"hours_worked\").as(\"total_hours\"),\n",
    "        round(avg(\"performance_rating\"), 2).as(\"avg_rating\"),\n",
    "        collect_list(\"employee_id\").as(\"contributors\")\n",
    "      )\n",
    "      .orderBy(desc(\"total_hours\"))\n",
    "  }\n",
    "}\n",
    "\n",
    "println(\"Industry DataFrame Configuration Loaded\")\n",
    "println()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Advanced SQL Transformations\n",
    "\n",
    "Production-grade SQL queries with window functions, CTEs, and analytical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Advanced SQL operations with window functions\n",
    "object AdvancedSQLTransformer {\n",
    "\n",
    "  // Create Delta table for time-series analytics\n",
    "  def createEmployeeTableDDL(spark: SparkSession): String =\n",
    "    \"\"\"\n",
    "    |CREATE TABLE IF NOT EXISTS employee_stats USING DELTA AS\n",
    "    |SELECT\n",
    "    |  e.id,\n",
    "    |  e.name,\n",
    "    |  e.department,\n",
    "    |  e.salary,\n",
    "    |  e.hire_date,\n",
    "    |  YEAR(e.hire_date) as hire_year,\n",
    "    |  FLOOR(DATEDIFF(CURRENT_DATE, e.hire_date) / 365.25) as years_experience,\n",
    "    |  CASE\n",
    "    |    WHEN e.salary < 50000 THEN 'Entry'\n",
    "    |    WHEN e.salary < 100000 THEN 'Mid'\n",
    "    |    WHEN e.salary < 150000 THEN 'Senior'\n",
    "    |    ELSE 'Executive'\n",
    "    |  END as salary_tier,\n",
    "    |  CURRENT_DATE as processed_date\n",
    "    |FROM json.`/data/employees/*.json`\n",
    "    |\"\"\".stripMargin\n",
    "\n",
    "  // Complex window function queries\n",
    "  def employeeRankingQueries(spark: SparkSession): Unit = {\n",
    "    import spark.implicits._\n",
    "\n",
    "    val employeeDF = spark.table(\"employee_stats\")\n",
    "\n",
    "    // Department-wise ranking and percentiles\n",
    "    val rankingsDF = employeeDF\n",
    "      .withColumn(\"dept_rank\",\n",
    "        row_number().over(\n",
    "          Window.partitionBy(\"department\").orderBy(desc(\"salary\"))\n",
    "        )\n",
    "      )\n",
    "      .withColumn(\"dept_percentile\",\n",
    "        percent_rank().over(\n",
    "          Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "        )\n",
    "      )\n",
    "      .withColumn(\"company_rank\",\n",
    "        dense_rank().over(\n",
    "          Window.orderBy(desc(\"salary\"))\n",
    "        )\n",
    "      )\n",
    "\n",
    "    val topPerformers = rankingsDF\n",
    "      .filter(\"dept_rank <= 2\")\n",
    "      .select(\n",
    "        \"department\",\n",
    "        \"name\",\n",
    "        \"salary\",\n",
    "        \"dept_rank\",\n",
    "        \"salary_tier\"\n",
    "      )\n",
    "      .orderBy(\"department\", \"dept_rank\")\n",
    "  }\n",
    "\n",
    "  // Analytical queries with CTEs\n",
    "  def departmentAnalytics(spark: SparkSession): DataFrame = {\n",
    "    spark.sql(\"\"\"\n",
    "      WITH department_metrics AS (\n",
    "        SELECT\n",
    "          department,\n",
    "          COUNT(*) as employee_count,\n",
    "          ROUND(AVG(salary), 2) as avg_salary,\n",
    "          ROUND(STDDEV(salary), 2) as salary_variation,\n",
    "          MIN(years_experience) as min_experience,\n",
    "          MAX(years_experience) as max_experience,\n",
    "          COUNT(DISTINCT salary_tier) as tier_diversity\n",
    "        FROM employee_stats\n",
    "        GROUP BY department\n",
    "      ),\n",
    "      ranked_departments AS (\n",
    "        SELECT\n",
    "          *,\n",
    "          ROW_NUMBER() OVER (ORDER BY employee_count DESC, avg_salary DESC) as dept_popularity_rank,\n",
    "          LAG(avg_salary) OVER (ORDER BY employee_count DESC) as prev_dept_avg_salary,\n",
    "          AVG(avg_salary) OVER (ORDER BY employee_count DESC\n",
    "                                ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as running_avg\n",
    "        FROM department_metrics\n",
    "      )\n",
    "      SELECT\n",
    "        *,\n",
    "        ROUND(CASE WHEN prev_dept_avg_salary IS NOT NULL\n",
    "                   THEN ((avg_salary - prev_dept_avg_salary) / prev_dept_avg_salary) * 100\n",
    "                   ELSE 0 END, 2) as growth_percent\n",
    "      FROM ranked_departments\n",
    "      ORDER BY dept_popularity_rank\n",
    "    \"\"\")\n",
    "  }\n",
    "\n",
    "  // Performance optimization queries\n",
    "  def queryOptimizationPatterns(spark: SparkSession): Unit = {\n",
    "\n",
    "    // 1. Predicate pushdown example\n",
    "    val filteredDF = spark.table(\"employee_stats\")\n",
    "      .filter(\"department = 'Engineering'\")\n",
    "      .filter(\"salary > 50000\")\n",
    "      .filter(\"years_experience > 3\")\n",
    "\n",
    "    // 2. Broadcast join for small tables\n",
    "    val departments = spark.table(\"departments\")\n",
    "      .select(\"dept_code\", \"dept_name\", \"budget\")\n",
    "      .hint(\"broadcast\")\n",
    "\n",
    "    // 3. Bucketing for frequent joins\n",
    "    spark.sql(\"\"\"\n",
    "      CREATE TABLE employee_buckets (\n",
    "        id INT,\n",
    "        name STRING,\n",
    "        department STRING,\n",
    "        salary DOUBLE\n",
    "      )\n",
    "      CLUSTERED BY (department) INTO 16 BUCKETS\n",
    "      STORED AS PARQUET\n",
    "    \"\"\"):\n",
    "\n",
    "    // 4. Skew hint for unbalanced data\n",
    "    val skewedJoin = spark.table(\"large_table\")\n",
    "      .hint(\"skew\", \"join\", Map(\"skewedColumn\" -> \"popular_value\"))\n",
    "      .join(skewedTable, \"common_column\")\n",
    "  }\n",
    "}\n",
    "\n",
    "println(\"Advanced SQL Transformations Configured\")\n",
    "println()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè≠ ETL Pipeline Design Pattern\n",
    "\n",
    "Production-ready ETL pipeline with error handling, monitoring, and data quality checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Enterprise ETL Pipeline Framework\n",
    "sealed trait ETLResult\n",
    "case class ETLStepResult(rowsProcessed: Long, duration: Long) extends ETLResult\n",
    "case class ETLError(step: String, error: Throwable, retryCount: Int) extends ETLResult\n",
    "case class ETLComplete(totalRows: Long, totalDuration: Long, steps: List[ETLStepResult]) extends ETLResult\n",
    "\n",
    "case class PipelineMetadata(\n",
    "  pipelineId: String,\n",
    "  executionId: String,\n",
    "  startTime: Long,\n",
    "  steps: List[String],\n",
    "  status: PipelineStatus\n",
    ")\n",
    "\n",
    "sealed trait PipelineStatus\n",
    "case object Running extends PipelineStatus\n",
    "case object Completed extends PipelineStatus\n",
    "case object Failed extends PipelineStatus\n",
    "case object Skipped extends PipelineStatus\n",
    "\n",
    "// Type-safe ETL step trait\n",
    "trait ETLStep[F[_]] {\n",
    "  def name: String\n",
    "  def description: String\n",
    "  def inputs: Set[String]\n",
    "  def outputs: Set[String]\n",
    "\n",
    "  def execute(metadata: PipelineMetadata): F[ETLResult]\n",
    "\n",
    "  // Pre/post execution hooks\n",
    "  def preExecute(metadata: PipelineMetadata): F[Unit] = unit\n",
    "  def postExecute(result: ETLResult, metadata: PipelineMetadata): F[Unit] = unit\n",
    "}\n",
    "\n",
    "// Concrete ETL steps\n",
    "class ExtractionStep[F[_]: Sync](config: ExtractionConfig) extends ETLStep[F] {\n",
    "  val name = \"extraction\"\n",
    "  val description = s\"Extract data from ${config.source}\"\n",
    "  val inputs = Set.empty[String]\n",
    "  val outputs = Set(\"raw_data\")\n",
    "\n",
    "  override def execute(metadata: PipelineMetadata): F[ETLResult] = {\n",
    "    for {\n",
    "      startTime <- timer.clock.realTime(MILLISECONDS)\n",
    "      df <- loadSourceData(config.source)\n",
    "      rowCount = df.count()\n",
    "      _ <- validateData(df, config.validations)\n",
    "      _ <- saveToTempTable(df, config.tempTableName)\n",
    "      endTime <- timer.clock.realTime(MILLISECONDS)\n",
    "    } yield ETLStepResult(rowCount, endTime - startTime)\n",
    "  }\n",
    "\n",
    "  private def loadSourceData(source: String): F[DataFrame] = {\n",
    "    val spark = getSparkSession()\n",
    "    source.split(\"::\") match {\n",
    "      case Array(\"csv\", path) => spark.read.csv(path)\n",
    "      case Array(\"parquet\", path) => spark.read.parquet(path)\n",
    "      case Array(\"delta\", path) => spark.read.format(\"delta\").load(path)\n",
    "      case Array(\"jdbc\", connString, table) =>\n",
    "        spark.read.jdbc(connString, table, Map.empty[String, String])\n",
    "      case _ => Sync[F].raiseError(new IllegalArgumentException(s\"Unsupported source: $source\"))\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "class TransformationStep[F[_]: Sync](config: TransformationConfig) extends ETLStep[F] {\n",
    "  val name = \"transformation\"\n",
    "  val description = \"Apply business logic transformations\"\n",
    "  val inputs = Set(\"raw_data\")\n",
    "  val outputs = Set(\"transformed_data\")\n",
    "\n",
    "  override def execute(metadata: PipelineMetadata): F[ETLResult] = {\n",
    "    for {\n",
    "      startTime <- timer.clock.realTime(MILLISECONDS)\n",
    "      rawDF <- loadFromTempTable(config.inputTable)\n",
    "      transformedDF <- applyTransforms(rawDF, config.transforms)\n",
    "      validatedDF <- runQualityChecks(transformedDF, config.qualityChecks)\n",
    "      _ <- saveToTransformedTable(validatedDF, config.outputTable)\n",
    "      rowCount = validatedDF.count()\n",
    "      endTime <- timer.clock.realTime(MILLISECONDS)\n",
    "    } yield ETLStepResult(rowCount, endTime - startTime)\n",
    "  }\n",
    "\n",
    "  private def applyTransforms(df: DataFrame, transforms: List[Transform]): F[DataFrame] = {\n",
    "    transforms.foldLeftM(df) { (currentDF, transform) =>\n",
    "      transform match {\n",
    "        case FilterTransform(condition) => currentDF.filter(condition)\n",
    "        case SelectTransform(columns) => currentDF.select(columns.map(col): _*)\n",
    "        case GroupByTransform(groupCols, aggCols) =>\n",
    "          currentDF.groupBy(groupCols.map(col): _*).agg(aggCols.map(col): _*)\n",
    "        case JoinTransform(otherTable, joinCols, joinType) =>\n",
    "          val otherDF = spark.table(otherTable)\n",
    "          currentDF.join(otherDF, joinCols, joinType)\n",
    "        case RenameTransform(renames) =>\n",
    "          renames.foldLeft(currentDF) { case (df, (old, new_)) =>\n",
    "            df.withColumnRenamed(old, new_)\n",
    "          }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "class LoadingStep[F[_]: Sync](config: LoadingConfig) extends ETLStep[F] {\n",
    "  val name = \"loading\"\n",
    "  val description = s\"Load data to ${config.destination}\"\n",
    "  val inputs = Set(\"transformed_data\")\n",
    "  val outputs = Set.empty[String]\n",
    "\n",
    "  override def execute(metadata: PipelineMetadata): F[ETLResult] = {\n",
    "    for {\n",
    "      startTime <- timer.clock.realTime(MILLISECONDS)\n",
    "      transformedDF <- loadFromTransformedTable(config.inputTable)\n",
    "      rowCount <- writeDestination(transformedDF, config.destination)\n",
    "      _ <- updateMetadataTables(metadata, rowCount)\n",
    "      endTime <- timer.clock.realTime(MILLISECONDS)\n",
    "    } yield ETLStepResult(rowCount, endTime - startTime)\n",
    "  }\n",
    "\n",
    "  private def writeDestination(df: DataFrame, destination: String): F[Long] = {\n",
    "    val writer = destination.split(\"::\") match {\n",
    "      case Array(\"delta\", path) =>\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"partition_date\")\n",
    "      case Array(\"parquet\", path) =>\n",
    "        df.write.mode(\"overwrite\").partitionBy(\"partition_date\").parquet(path)\n",
    "      case Array(\"hive\", database, table) =>\n",
    "        df.write.mode(\"append\").saveAsTable(s\"$database.$table\")\n",
    "      case Array(\"jdbc\", connString, table) =>\n",
    "        df.write.jdbc(connString, table, Map.empty[String, String])\n",
    "    }\n",
    "\n",
    "    writer.save(destination)\n",
    "    df.count().pure[F]\n",
    "  }\n",
    "}\n",
    "\n",
    "// Pipeline orchestration\n",
    "class ETLPipeline[F[_]: Async: Parallel](\n",
    "  steps: List[ETLStep[F]],\n",
    "  metadataStore: MetadataStore[F],\n",
    "  errorHandler: ErrorHandler[F]\n",
    ")(implicit timer: Timer[F]) {\n",
    "\n",
    "  def execute(): F[ETLComplete] = {\n",
    "    for {\n",
    "      metadata <- createPipelineMetadata()\n",
    "      _ <- metadataStore.save(metadata)\n",
    "\n",
    "      results <- steps.zipWithIndex.traverse { case (step, index) =>\n",
    "        val stepMetadata = metadata.copy(\n",
    "          steps = metadata.steps.take(index + 1),\n",
    "          status = Running\n",
    "        )\n",
    "\n",
    "        step.preExecute(stepMetadata) >>\n",
    "        step.execute(stepMetadata).attempt.flatMap {\n",
    "          case Right(result) =>\n",
    "            metadataStore.updateStepSuccess(metadata, step.name, result) >>\n",
    "            step.postExecute(result, stepMetadata) >>\n",
    "            result.pure[F]\n",
    "          case Left(error) =>\n",
    "            metadataStore.updateStepFailure(metadata, step.name, error) >>\n",
    "            errorHandler.handleETLError(error, step, stepMetadata) >>\n",
    "            error.pure[F]\n",
    "        }\n",
    "      }\n",
    "\n",
    "      successResults = results.collect { case r: ETLStepResult => r }\n",
    "      totalRows = successResults.map(_.rowsProcessed).sum\n",
    "      totalDuration = successResults.map(_.duration).sum\n",
    "\n",
    "      _ <- metadataStore.save(\n",
    "        metadata.copy(status = Completed, startTime = System.currentTimeMillis())\n",
    "      )\n",
    "\n",
    "    } yield ETLComplete(totalRows, totalDuration, successResults)\n",
    "  }\n",
    "}\n",
    "\n",
    "println(\"Industry ETL Pipeline Framework Loaded\")\n",
    "println(\"Features: Type Safety, Error Handling, Parallel Processing, Monitoring\")\n",
    "println()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Performance Tuning & Optimization\n",
    "\n",
    "Advanced techniques used by production Spark teams for maximum performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Advanced Performance Tuning for Enterprise Spark\n",
    "object SparkPerformanceTuning {\n",
    "\n",
    "  case class TuningConfig(\n",
    "    executorCores: Int = 4,\n",
    "    executorMemoryGB: Double = 8.0,\n",
    "    driverMemoryGB: Double = 2.0,\n",
    "    parallelism: Int = 200,\n",
    "    compressionCodec: String = \"snappy\",\n",
    "    fileFormat: String = \"parquet\"\n",
    "  )\n",
    "\n",
    "  // Optimal configuration calculator\n",
    "  def calculateOptimalConfig(clusterSpec: ClusterSpec, workloadType: WorkloadType): TuningConfig = {\n",
    "    val coresPerExecutor = workloadType match {\n",
    "      case CPUIntensive => 1 // Maximize parallelism\n",
    "      case MemoryIntensive => math.max(2, clusterSpec.totalCores / clusterSpec.numExecutors)\n",
    "      case IOIntensive => 4 // Balance CPU and I/O\n",
    "      case BalancedWorkload => 2\n",
    "    }\n",
    "\n",
    "    val memoryPerExecutor = workloadType match {\n",
    "      case MemoryIntensive => clusterSpec.memoryPerNode * 0.75\n",
    "      case _ => (clusterSpec.memoryPerNode / 8) * coresPerExecutor\n",
    "    }\n",
    "\n",
    "    val shufflePartitions = workloadType match {\n",
    "      case CPUIntensive | BalancedWorkload => clusterSpec.totalCores * 3\n",
    "      case MemoryIntensive | IOIntensive => clusterSpec.totalCores * 2\n",
    "    }\n",
    "\n",
    "    TuningConfig(\n",
    "      executorCores = coresPerExecutor,\n",
    "      executorMemoryGB = memoryPerExecutor,\n",
    "      driverMemoryGB = clusterSpec.memoryPerNode * 0.1,\n",
    "      parallelism = shufflePartitions,\n",
    "      compressionCodec = workloadType match {\n",
    "        case IOIntensive => \"lz4\" // Fast compression\n",
    "        case _ => \"snappy\" // Good balance\n",
    "      },\n",
    "      fileFormat = \"parquet\" // Columnar format for analytics\n",
    "    )\n",
    "  }\n",
    "\n",
    "  // Adaptive query execution optimization\n",
    "  def optimizeQueryAdaptive(spark: SparkSession, df: DataFrame): DataFrame = {\n",
    "    // Enable Adaptive Query Execution\n",
    "    spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "    spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "    spark.conf.set(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\")\n",
    "\n",
    "    df.repartition() // Will adaptively coalesce based on sizes\n",
    "      .hint(\"adaptive\")\n",
    "  }\n",
    "\n",
    "  // Dynamic allocation optimization\n",
    "  def configureDynamicAllocation(spark: SparkSession): Unit = {\n",
    "    spark.conf.set(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "    spark.conf.set(\"spark.dynamicAllocation.minExecutors\", \"2\")\n",
    "    spark.conf.set(\"spark.dynamicAllocation.maxExecutors\", \"10\")\n",
    "    spark.conf.set(\"spark.dynamicAllocation.executorIdleTimeout\", \"60s\")\n",
    "    spark.conf.set(\"spark.dynamicAllocation.cachedExecutorIdleTimeout\", \"300s\")\n",
    "  }\n",
    "\n",
    "  // Cache-aware optimizations\n",
    "  def optimizeCaching(spark: SparkSession, df: DataFrame, accessPattern: AccessPattern): DataFrame = {\n",
    "    val storageLevel = accessPattern match {\n",
    "      case FrequentlyAccessed => StorageLevel.MEMORY_AND_DISK\n",
    "      case ReadOnce => StorageLevel.MEMORY_ONLY // Will spill to disk if needed\n",
    "      case ComputeIntensive => StorageLevel.MEMORY_AND_DISK_SER\n",
    "      case SmallDataset => StorageLevel.MEMORY_ONLY_DESER\n",
    "    }\n",
    "\n",
    "    df.persist(storageLevel)\n",
    "\n",
    "    // Pre-compute aggregates if frequently accessed\n",
    "    if (accessPattern == FrequentlyAccessed) {\n",
    "      df.groupBy(\"key_field\").agg(count(\"*\"), avg(\"numeric_field\"), max(\"timestamp_field\"))\n",
    "    }\n",
    "\n",
    "    df\n",
    "  }\n",
    "\n",
    "  // Broadcast join optimization for small tables\n",
    "  def optimizeBroadcastJoin[\n",
    "    spark: SparkSession,\n",
    "    largeDF: DataFrame,\n",
    "    smallDF: DataFrame,\n",
    "    joinKeys: Seq[String]\n",
    "  ]: DataFrame = {\n",
    "    val smallSize = smallDF.count()\n",
    "    val threshold = spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\").toLong\n",
    "\n",
    "    if (smallSize > threshold) {\n",
    "      logger.warn(s\"Table size $smallSize exceeds broadcast threshold $threshold\")\n",
    "      // Consider bucketing, salting, or other optimizations\n",
    "    }\n",
    "\n",
    "    largeDF.hint(\"merge\")  // Prefer sort-merge join for large tables\n",
    "            .join(broadcast(smallDF), joinKeys, \"left\")\n",
    "  }\n",
    "\n",
    "  // Skew handling strategies\n",
    "  def handleDataSkew(spark: SparkSession, df: DataFrame, skewColumns: Seq[String]): DataFrame = {\n",
    "\n",
    "    // Technique 1: Salting (add prefix to high-frequency keys)\n",
    "    val saltFactor = 4\n",
    "    val saltedDF = df.withColumn(\"salt_key\",\n",
    "      when(col(\"key\").isin(skewColumns: _*), concat(col(\"key\"), lit(\"_\"), rand() % saltFactor))\n",
    "      .otherwise(col(\"key\"))\n",
    "    )\n",
    "\n",
    "    // Technique 2: Pre-aggregation for skewed groups\n",
    "    skewedColumns.foreach { skewCol =>\n",
    "      saltedDF.filter(col(\"key\") === skewCol)\n",
    "        .groupBy(\"key\")\n",
    "        .agg(collect_list(\"value\").as(\"aggregated_values\"))\n",
    "        .persist()\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // Column pruning and pushdown optimization\n",
    "  def optimizeColumnSelection(spark: SparkSession, df: DataFrame): DataFrame = {\n",
    "    df.select(\"needed_col1\", \"needed_col2\")  // Early column pruning\n",
    "      .filter(\"filter_on_partitioned_column = 'specific_value'\")  // Partition pruning\n",
    "      .repartition(col(\"frequently_joined_column\"))  // Partition for join\n",
    "  }\n",
    "}\n",
    "\n",
    "// Performance monitoring and alerting\n",
    "class SparkMetricsCollector(spark: SparkSession) {\n",
    "\n",
    "  def collectJobMetrics(jobId: Long): Map[String, Any] = {\n",
    "    val job = spark.sparkContext.statusTracker.getJobInfo(jobId).get\n",
    "\n",
    "    Map(\n",
    "      \"job_id\" -> jobId,\n",
    "      \"stages_count\" -> job.numStages,\n",
    "      \"active_stages\" -> job.numActiveStages,\n",
    "      \"failed_stages\" -> job.numFailedStages,\n",
    "      \"completed_tasks\" -> job.numCompletedTasks,\n",
    "      \"active_tasks\" -> job.numActiveTasks,\n",
    "      \"failed_tasks\" -> job.numFailedTasks\n",
    "    )\n",
    "  }\n",
    "\n",
    "  def collectStageMetrics(stageId: Int): Map[String, Any] = {\n",
    "    val stage = spark.sparkContext.statusTracker.getStageInfo(stageId).get\n",
    "\n",
    "    Map(\n",
    "      \"stage_id\" -> stageId,\n",
    "      \"tasks_total\" -> stage.numTasks,\n",
    "      \"tasks_completed\" -> stage.numCompletedTasks,\n",
    "      \"tasks_active\" -> stage.numActiveTasks,\n",
    "      \"tasks_failed\" -> stage.numFailedTasks,\n",
    "      \"bytes_read\" -> stage.rddInfos.map(_.memSize).sum,\n",
    "      \"bytes_written\" -> stage.stageStats.map(_.shuffleWriteBytesWritten).sum,\n",
    "      \"duration\" -> stage.submissionTime.map(t => System.currentTimeMillis() - t).getOrElse(0L)\n",
    "    )\n",
    "  }\n",
    "\n",
    "  def alertIfSlow(thresholdMs: Long): Unit = {\n",
    "    val jobs = spark.sparkContext.statusTracker.getJobIds()\n",
    "    jobs.foreach { jobId =>\n",
    "      val info = spark.sparkContext.statusTracker.getJobInfo(jobId).get\n",
    "      val duration = info.submissionTime.map(t => System.currentTimeMillis() - t).getOrElse(0L)\n",
    "\n",
    "      if (duration > thresholdMs) {\n",
    "        logger.warn(s\"Job $jobId is running slow (${duration}ms), check query plan\")\n",
    "        // Send alerts, log query plan, etc.\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  def analyzeQueryPlan(sql: String): Map[String, String] = {\n",
    "    val explained = spark.sql(sql).queryExecution.explainedString\n",
    "    Map(\n",
    "      \"has_broadcast_join\" -> explained.contains(\"broadcast\").toString,\n",
    "      \"has_sort_merge_join\" -> explained.contains(\"sort-merge\").toString,\n",
    "      \"has_shuffle\" -> explained.contains(\"Exchange hashpartitioning\").toString,\n",
    "      \"estimated_size\" -> explained.linesIterator.find(_.contains(\"size\")).getOrElse(\"unknown\"),\n",
    "      \"query_plan\" -> explained\n",
    "    )\n",
    "  }\n",
    "}\n",
    "\n",
    "println(\"Industry Performance Tuning Framework Loaded\")\n",
    "println(\"Includes: Adaptive Execution, Dynamic Allocation, Skew Handling, Monitoring\")\n",
    "println(\"Used by Netflix, Airbnb, and other major Spark deployments\")\n",
    "println()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Industry DataFrame Best Practices\n",
    "\n",
    "### **Catalyst Optimizer Understanding**\n",
    "- **Logical Plan**: Initial parsed plan with unused nodes\n",
    "- **Physical Plans**: Multiple execution strategies\n",
    "- **Cost-Based Optimization**: Statistics-based plan selection\n",
    "- **Whole-Stage Code Generation**: JIT compilation for performance\n",
    "\n",
    "### **Tungsten Execution Engine**\n",
    "- **Memory Management**: Custom off-heap memory manager\n",
    "- **Cache-Aware Computation**: NUMA-aware data placement\n",
    "- **Vectorization**: SIMD operations for modern CPUs\n",
    "- **Code Generation**: Dynamic bytecode generation\n",
    "\n",
    "### **Delta Lake Integration**\n",
    "- **ACID Transactions**: Multi-table transaction support\n",
    "- **Schema Enforcement**: Prevent corrupt data writes\n",
    "- **Time Travel**: Query historical table states\n",
    "- **Optimized Layouts**: Data skipping and Z-Ordering\n",
    "\n",
    "### **Structured Streaming**\n",
    "- **Micro-Batch vs Continuous**: Processing mode selection\n",
    "- **Event Time Processing**: Out-of-order event handling\n",
    "- **Watermarking**: Late data dropping strategy\n",
    "- **State Store**: Checkpointing and fault tolerance\n",
    "\n",
    "**Next: Real production data lake patterns and architectures**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
